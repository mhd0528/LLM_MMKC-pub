{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184d5ce6-8c15-442b-9cfb-874177a3318d",
   "metadata": {},
   "source": [
    "#### Test InstructBLIP_t5 model on one entity and triple generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59a59a01-cf2b-4648-919b-7a92d818dbe2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Blip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "print(\"Init Blip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec43192f-8d4a-4e9e-af49-5f3e5ec319f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: conda: command not found\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fe8a66e-cb56-484f-b0ce-e8a5f99f1e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80cb991-9cf2-4013-8fc8-4fc6aeb4bfdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# load model that can take multiple images\n",
    "from model.instructblip import (\n",
    "    InstructBlipConfig, InstructBlipModel, InstructBlipPreTrainedModel,InstructBlipForConditionalGeneration,InstructBlipProcessor\n",
    ")\n",
    "print(\"Init Blip 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67226afc-74a8-4d2f-9477-20193096927a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'InstructBlipConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model_ckpt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBleachNick/MMICL-Instructblip-T5-xxl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m processor_ckpt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalesforce/instructblip-flan-t5-xxl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mInstructBlipConfig\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      8\u001b[0m     model_ckpt, \n\u001b[1;32m      9\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mCACHE_DIR\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m InstructBlipForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     14\u001b[0m     model_ckpt,\n\u001b[1;32m     15\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     16\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mCACHE_DIR\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m processor \u001b[38;5;241m=\u001b[39m InstructBlipProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     20\u001b[0m     processor_ckpt,\n\u001b[1;32m     21\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mCACHE_DIR\n\u001b[1;32m     22\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'InstructBlipConfig' is not defined"
     ]
    }
   ],
   "source": [
    "CACHE_DIR = '/blue/daisyw/dkasinets/envs/cache/'\n",
    "\n",
    "model_type=\"instructblip\"\n",
    "model_ckpt=\"BleachNick/MMICL-Instructblip-T5-xxl\"\n",
    "processor_ckpt = \"Salesforce/instructblip-flan-t5-xxl\"\n",
    "\n",
    "config = InstructBlipConfig.from_pretrained(\n",
    "    model_ckpt, \n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "\n",
    "model = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "    model_ckpt,\n",
    "    config=config,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "processor = InstructBlipProcessor.from_pretrained(\n",
    "    processor_ckpt,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6d31e-3f4c-4194-8cea-67627bdd2357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ignore\n",
    "image_palceholder=\"å›¾\"\n",
    "sp = [image_palceholder]+[f\"<image{i}>\" for i in range(20)]\n",
    "sp = sp+processor.tokenizer.additional_special_tokens[len(sp):]\n",
    "processor.tokenizer.add_special_tokens({'additional_special_tokens':sp})\n",
    "if model.qformer.embeddings.word_embeddings.weight.shape[0] != len(processor.qformer_tokenizer):\n",
    "    model.qformer.resize_token_embeddings(len(processor.qformer_tokenizer))\n",
    "\n",
    "# init replace token\n",
    "replace_token=\"\".join(32*[image_palceholder])\n",
    "\n",
    "print(f\"sp: {sp}\")\n",
    "print(f\"replace_token: {replace_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b9ac8-1d08-4c83-952c-45a60b8c64a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kg_img_context_folder = \"/orange/daisyw/ma.haodi/LLM-MMKGC/data/FB15k-237/\"\n",
    "image_folder = os.path.join(kg_img_context_folder, \"image-graph_images\")\n",
    "print(\"Images location in Haodi's dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7d00a8e-5284-471b-83b0-f9f461163104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context_json_dir = \"/blue/daisyw/dkasinets/mmkgc/LLM-MMKGC/context_generation/context_json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20776f4-799e-4dd3-aae1-74a69974b9c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### generation for triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ae209b-bfbc-46e9-be1c-d246804818c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set triples and ent2str_dict\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# data_folder = \"/blue/daisyw/ma.haodi/LLM-MMKGC/data/fb15k-237/\"\n",
    "data_folder = \"/blue/daisyw/ma.haodi/LLM-MMKGC/data/fb15k-237/\"\n",
    "train_triples_file = \"train.txt\"\n",
    "\n",
    "# triples\n",
    "train_triples = []\n",
    "train_ent_dict = collections.defaultdict(list)\n",
    "with open(os.path.join(data_folder, train_triples_file)) as triple_f:\n",
    "    for line in triple_f.readlines():\n",
    "        obj, rel, subj = line.strip().split(\"\\t\")\n",
    "        train_triples.append([obj, rel, subj])\n",
    "        train_ent_dict[obj].append([rel, subj])\n",
    "        train_ent_dict[subj].append([rel, obj])\n",
    "        \n",
    "\n",
    "# ent id 2 text\n",
    "# load id2txt into dictionary\n",
    "ent2str_file = \"entity_strings.del\"\n",
    "ent2str_dict = {}\n",
    "with open(os.path.join(data_folder, ent2str_file)) as e2s_f:\n",
    "    for line in e2s_f.readlines():\n",
    "        e_id, e_str = line.strip().split(\"\\t\")\n",
    "        ent2str_dict[e_id] = e_str\n",
    "\n",
    "print(\"Set triples and ent2str_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "952324c0-d157-41e3-83f6-e698c2d31963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to get probabilities\n",
    "# Get model output as token (for Yes and No), return Yes and No probabilities. \n",
    "def get_yes_no_probabilities(generated_outputs):\n",
    "    # Extract logits for each generation step\n",
    "    token_scores = generated_outputs.scores  # List of logits for each step\n",
    "    \n",
    "    # Ensure that there's at least one step\n",
    "    if len(token_scores) == 0:\n",
    "        print(\"No token scores available.\")\n",
    "        return None, None, generated_sentence\n",
    "    \n",
    "    # Extract logits for the first token\n",
    "    first_step_logits = token_scores[0]  # Tensor of shape (batch_size, vocab_size)\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    first_step_probs = torch.softmax(first_step_logits, dim=-1)\n",
    "    \n",
    "    # Extract token IDs for \"Yes\", \"yes\", \"No\", \"no\"\n",
    "    yes_token_ids = [\n",
    "        processor.tokenizer.encode(\"Yes\", add_special_tokens=False)[0],\n",
    "        processor.tokenizer.encode(\"yes\", add_special_tokens=False)[0]\n",
    "    ]\n",
    "    no_token_ids = [\n",
    "        processor.tokenizer.encode(\"No\", add_special_tokens=False)[0],\n",
    "        processor.tokenizer.encode(\"no\", add_special_tokens=False)[0]\n",
    "    ]\n",
    "    \n",
    "    # Calculate combined probabilities for \"Yes\"/\"yes\" and \"No\"/\"no\"\n",
    "    yes_probability = sum(first_step_probs[0, token_id].item() for token_id in yes_token_ids)\n",
    "    no_probability = sum(first_step_probs[0, token_id].item() for token_id in no_token_ids)\n",
    "    \n",
    "    return yes_probability, no_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d237ea9b-d831-4110-bf15-83627cee37db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prob_with_sentence(processor, model, prompt, image):\n",
    "    images = [image]\n",
    "    inputs = processor(images=images, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "    inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "    print(inputs['pixel_values'].shape)\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "                pixel_values = inputs['pixel_values'],\n",
    "                input_ids = inputs['input_ids'],\n",
    "                attention_mask = inputs['attention_mask'],\n",
    "                img_mask = inputs['img_mask'],\n",
    "                do_sample=False,\n",
    "                max_length=50,\n",
    "                min_length=1,\n",
    "                set_min_padding_size =False,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "    # extract text and yes/no probability\n",
    "    generated_text = processor.batch_decode(\n",
    "        outputs.sequences, skip_special_tokens=True\n",
    "        )[0].strip()\n",
    "    \n",
    "    # use yes/no probability as confidence\n",
    "    yes_prob, no_prob = get_yes_no_probabilities(outputs)\n",
    "    \n",
    "    return yes_prob, no_prob, generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48df6987-7ec6-497a-8c6c-9dfb593387a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def related_ent_img_search(ent_path, model, processor, prompt_list):\n",
    "    context_list = []\n",
    "    prompt_1, prompt_2, context_prompt = prompt_list\n",
    "    # print(prompt_1, prompt_2, context_prompt)\n",
    "    for image_path in os.listdir(ent_path):\n",
    "        yes_probs, no_probs = [], []\n",
    "        image_full_path = os.path.join(ent_path, image_path)\n",
    "        with open(image_full_path) as f:\n",
    "            # print(f\"Content of '{image_full_path}'\")\n",
    "            image = Image.open(image_full_path).convert(\"RGB\")\n",
    "            # resize image if its size is 1, 1\n",
    "            if image.size == (1, 1):\n",
    "                print(\"\\t>>>>Resizing the image, original size is 1*1, will cause encoder issue...\")\n",
    "                image = image.resize((10, 10))\n",
    "\n",
    "            # generation and compute prob for obj\n",
    "            yes_prob, no_prob, generated_text = get_prob_with_sentence(processor, model, prompt_1, image)\n",
    "            yes_probs.append(yes_prob)\n",
    "            no_probs.append(no_prob)\n",
    "            # log generated info\n",
    "\n",
    "            # generation and compute prob for sub\n",
    "            yes_prob, no_prob, generated_text = get_prob_with_sentence(processor, model, prompt_2, image)\n",
    "            yes_probs.append(yes_prob)\n",
    "            no_probs.append(no_prob)\n",
    "            \n",
    "            # generate additional context if both entity exists\n",
    "            if yes_probs[0] > no_probs[0] and yes_probs[1] > no_probs[1]:\n",
    "                context_inputs = processor(\n",
    "                    images=image, text=context_prompt, return_tensors=\"pt\"\n",
    "                    ).to(device, torch.bfloat16)\n",
    "                with torch.no_grad():\n",
    "                    context_outputs = model.generate(\n",
    "                            **context_inputs,\n",
    "                            do_sample=True,  # generate with sample may return conflict with prob\n",
    "                            # num_beams=5,\n",
    "                            max_length=1024,\n",
    "                            min_length=1,\n",
    "                            top_p=0.9,\n",
    "                            repetition_penalty=1.5,\n",
    "                            length_penalty=1.0,\n",
    "                            temperature=1,  # generate with greedy or not\n",
    "                            output_scores=True,\n",
    "                            return_dict_in_generate=True\n",
    "                    )\n",
    "\n",
    "                # extract text and yes/no probability\n",
    "                context_text = processor.batch_decode(\n",
    "                    context_outputs.sequences, skip_special_tokens=True\n",
    "                    )[0].strip()\n",
    "                context_list.append(context_text)\n",
    "                \n",
    "                print(f\"\\t>>>> Image: {image_path} | Yes: {[round(yes_prob, 6) for yes_prob in yes_probs]}, No: {[round(no_prob, 6) for no_prob in no_probs]}\")\n",
    "                print(f\"\\t>>>> Generated context: {context_text}\\n\")\n",
    "    return context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00e9b547-b5aa-4270-a8f3-df892208ad57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate summaries\n",
    "def get_useful_images(ent_path, model, processor, prompt_list, select_best_one=False):\n",
    "    prompt_1, prompt_2, context_prompt = prompt_list\n",
    "    \n",
    "    useful_images = []\n",
    "    best_image = None\n",
    "    best_yes_prob = float('-inf')  # Initialize to negative infinity for comparison\n",
    "    best_no_prob = None\n",
    "    \n",
    "    for image_path in os.listdir(ent_path):\n",
    "        yes_probs, no_probs = [], []\n",
    "        image_full_path = os.path.join(ent_path, image_path)\n",
    "\n",
    "        with open(image_full_path) as f:\n",
    "            image = Image.open(image_full_path)\n",
    "            \n",
    "            # Resize image if its size is 1, 1\n",
    "            if image.size == (1, 1):\n",
    "                # print(\"\\t>>>>Resizing the image, original size is 1*1, will cause encoder issue...\")\n",
    "                image = image.resize((10, 10))\n",
    "            \n",
    "            # Generation and compute prob for obj\n",
    "            yes_prob_1, no_prob_1, generated_text = get_prob_with_sentence(processor, model, prompt_1, image)\n",
    "            yes_probs.append(yes_prob_1)\n",
    "            no_probs.append(no_prob_1)\n",
    "\n",
    "            # Generation and compute prob for sub\n",
    "            yes_prob_2, no_prob_2, generated_text = get_prob_with_sentence(processor, model, prompt_2, image)\n",
    "            yes_probs.append(yes_prob_2)\n",
    "            no_probs.append(no_prob_2)\n",
    "\n",
    "            # Generate additional context if both entities exist\n",
    "            if yes_probs[0] > no_probs[0] and yes_probs[1] > no_probs[1]:\n",
    "                if select_best_one:\n",
    "                    # Check if current image has the highest yes probability\n",
    "                    total_yes_prob = sum(yes_probs)\n",
    "                    if total_yes_prob > best_yes_prob:\n",
    "                        best_yes_prob = total_yes_prob\n",
    "                        best_no_prob = sum(no_probs)\n",
    "                        best_image = image\n",
    "                else:\n",
    "                    useful_images.append(image)\n",
    "                    print(f\"\\t>>>> Image: {image_path} | Yes: {[round(yes_prob, 6) for yes_prob in yes_probs]}, No: {[round(no_prob, 6) for no_prob in no_probs]}\")\n",
    "\n",
    "    # Add the best image to useful_images if select_best_one is True\n",
    "    if select_best_one and best_image is not None:\n",
    "        useful_images.append(best_image)\n",
    "        print(f\"\\t>>>> Best Image: {image_path} | Yes: {round(best_yes_prob, 6)}, Yes: {round(best_no_prob, 6)}\")\n",
    "        \n",
    "    if len(useful_images) == 0:\n",
    "        print(f\"\\t>>>> No useful images!\")\n",
    "    \n",
    "    return useful_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fb14c8f-ce00-47e7-a51c-ac7b6dbcc9a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Summarize given set of images \n",
    "def get_summary_context(useful_images, summary_prompt):\n",
    "    context_list = []\n",
    "    \n",
    "    # TODO\n",
    "    inputs = processor(images=useful_images, text=summary_prompt, return_tensors=\"pt\")\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "    inputs['img_mask'] = torch.tensor([[1 for i in range(len(useful_images))]])\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "    \n",
    "    inputs = inputs.to('cuda:0')\n",
    "    with torch.no_grad():\n",
    "        context_outputs = model.generate(\n",
    "                pixel_values = inputs['pixel_values'],\n",
    "                input_ids = inputs['input_ids'],\n",
    "                attention_mask = inputs['attention_mask'],\n",
    "                img_mask = inputs['img_mask'],\n",
    "                do_sample=False,\n",
    "                max_length=50,\n",
    "                min_length=1,\n",
    "                set_min_padding_size =False,\n",
    "        )\n",
    "    \n",
    "    context_text = processor.batch_decode(\n",
    "        context_outputs, skip_special_tokens=True\n",
    "        )[0].strip()\n",
    "    \n",
    "    context_list.append(context_text)\n",
    "    display(HTML(f'\\t>>>> Generated context: <p style=\"color:green;\">{context_text}</p>'))\n",
    "    \n",
    "    return context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3714301a-4c9b-408e-96f1-ee93ffb23477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_image_relation_prompt(all_useful_images, obj_s, subj_s):\n",
    "    #     Example (of what we want):\n",
    "\n",
    "    #     query: Yambao (obj) | genre |\n",
    "    #     context:\n",
    "    #     instance of film | film\n",
    "    #     country of origin | Mexico\n",
    "    #     reverse of directed | Alfredo B Crevenna \n",
    "\n",
    "    #     â€¦ \n",
    "    #     We want:\n",
    "    #     takes place in | Cuba (subj)\n",
    "    \n",
    "    if len(all_useful_images) == 0:\n",
    "        return None\n",
    "    \n",
    "    token_replace_str = \" \".join([f\"image {idx}: <image{idx}>{replace_token}\" + (\",\" if idx < len(all_useful_images) - 1 else \"\") for idx in range(len(all_useful_images))])\n",
    "    prompt = (\n",
    "        f\"Analyze the set of images {token_replace_str}\"\n",
    "        \"The model should primarily rely on the visual information in the images to understand the context.\"\n",
    "        \"In one complete, longer sentence, what is the relationship represented in the set of images?\"\n",
    "        \"Don't provide trivial information. Only return useful information from the set of images.\"\n",
    "        f\"if entity {obj_s} and entity {subj_s} are not presented in the set of images, state that no relationship was found.\"\n",
    "    )\n",
    "    # prompt = (\n",
    "    #     f\"Analyze the set of images {token_replace_str} for non-trivial information related to entity {obj_s} and entity {subj_s}.\"\n",
    "    #     f\"Based on the extracted information, return one sentence explaining how the entity {obj_s} relates to the entity {subj_s}.\"\n",
    "    #     \"If no meaningful connection can be established, state that no relevant relationship was found.\"\n",
    "    #     \"if subject and object are not presented in the set of images, state that no relationship was found.\"\n",
    "    #     \"Don't use information purely from provided entity labels, make sure to use image information for relationships.\"\n",
    "    # )\n",
    "   \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e9f25ab-be7b-42c0-9984-7fd169ed666d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "Generation about /m/027rn: Dominican_Republic and /m/06cx9: Republic, relation: /location/country/form_of_government\n",
      "\t======== Processing object /m/027rn images...\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "\t>>>> Image: google_10.jpg | Yes: [0.467216, 0.480831], No: [0.168977, 0.179717]\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "\t======== Processing subject /m/06cx9 images...\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(subj_path):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m======== Processing subject \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m     subj_images_list \u001b[38;5;241m=\u001b[39m \u001b[43mget_useful_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubj_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselect_best_one\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m======== Subject \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m, in \u001b[0;36mget_useful_images\u001b[0;34m(ent_path, model, processor, prompt_list, select_best_one)\u001b[0m\n\u001b[1;32m     25\u001b[0m no_probs\u001b[38;5;241m.\u001b[39mappend(no_prob_1)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Generation and compute prob for sub\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m yes_prob_2, no_prob_2, generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mget_prob_with_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m yes_probs\u001b[38;5;241m.\u001b[39mappend(yes_prob_2)\n\u001b[1;32m     30\u001b[0m no_probs\u001b[38;5;241m.\u001b[39mappend(no_prob_2)\n",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m, in \u001b[0;36mget_prob_with_sentence\u001b[0;34m(processor, model, prompt, image)\u001b[0m\n\u001b[1;32m     10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 13\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpixel_values\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimg_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mset_min_padding_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# extract text and yes/no probability\u001b[39;00m\n\u001b[1;32m     27\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m     28\u001b[0m     outputs\u001b[38;5;241m.\u001b[39msequences, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/LLM-MMKGC/context_generation/model/instructblip/modeling_instructblip.py:2157\u001b[0m, in \u001b[0;36mInstructBlipForConditionalGeneration.generate\u001b[0;34m(self, pixel_values, qformer_input_ids, qformer_attention_mask, input_ids, attention_mask, img_mask, set_min_padding_size, sp_token, **generate_kwargs)\u001b[0m\n\u001b[1;32m   2151\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask[:,:min_padding_size]\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;66;03m# concatenate query embeddings with prompt embeddings\u001b[39;00m\n\u001b[0;32m-> 2157\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2160\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/transformers/generation/utils.py:1745\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_attention_mask_for_generation(\n\u001b[1;32m   1740\u001b[0m         inputs_tensor, generation_config\u001b[38;5;241m.\u001b[39m_pad_token_tensor, generation_config\u001b[38;5;241m.\u001b[39m_eos_token_tensor\n\u001b[1;32m   1741\u001b[0m     )\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1744\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/transformers/generation/utils.py:549\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    547\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    548\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 549\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1106\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1092\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1093\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1103\u001b[0m         output_attentions,\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1106\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:686\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    684\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    696\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:593\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    584\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    591\u001b[0m ):\n\u001b[1;32m    592\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 593\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    603\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/torch/nn/modules/module.py:1514\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1514\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We want this: Generating for triples\n",
    "\n",
    "import collections\n",
    "from pathlib import Path\n",
    "import json as js\n",
    "\n",
    "# for each triple, we generate the caption based on the entity\n",
    "# the idea is to select images that are about the triple\n",
    "triple_cnt = 0\n",
    "resume = 0\n",
    "stop = -1\n",
    "\n",
    "if stop == -1:\n",
    "    stop = len(train_triples)\n",
    "\n",
    "# save generations to a dicitonary\n",
    "ent_context_dict = dict()\n",
    "\n",
    "for idx, (obj, rel, subj) in enumerate(train_triples):\n",
    "    if idx < resume:\n",
    "        continue\n",
    "    if stop != -1 and idx > stop:\n",
    "        break\n",
    "\n",
    "    triple_cnt += 1\n",
    "\n",
    "    obj_s, subj_s = ent2str_dict[obj], ent2str_dict[subj]\n",
    "    obj_path = os.path.join(image_folder, obj[1:].replace('/', '.'))\n",
    "    subj_path = os.path.join(image_folder, subj[1:].replace('/', '.'))\n",
    "    print(\"========\")\n",
    "    \n",
    "    # Skip because we want only a single relation for subject and object (we don't care for same subject/object haveing multiple relations)\n",
    "    # Our task is adding captions to subject/object. \n",
    "    # skip if we already processed this triple\n",
    "    if (obj in ent_context_dict) and (subj in ent_context_dict[obj][\"context_dict\"]):\n",
    "        print(f\"Triples between {obj}:{obj_s} and {subj}:{subj_s} already processed... Skip...\")\n",
    "        print(\"========\\n\")\n",
    "        continue\n",
    "        \n",
    "    # initialize context dictionary for obj and subj\n",
    "    if obj not in ent_context_dict:\n",
    "        ent_context_dict[obj] = dict()\n",
    "        ent_context_dict[obj][\"ent_str\"] = obj_s\n",
    "        ent_context_dict[obj][\"context_dict\"] = collections.defaultdict(list)\n",
    "    if subj not in ent_context_dict[obj][\"context_dict\"]:\n",
    "        ent_context_dict[obj][\"context_dict\"][subj] = []\n",
    "    if subj not in ent_context_dict:\n",
    "        ent_context_dict[subj] = dict()\n",
    "        ent_context_dict[subj][\"ent_str\"] = subj_s\n",
    "        ent_context_dict[subj][\"context_dict\"] = collections.defaultdict(list)\n",
    "    if obj not in ent_context_dict[subj][\"context_dict\"]:\n",
    "        ent_context_dict[subj][\"context_dict\"][obj] = []\n",
    "    \n",
    "    print(f\"Generation about {obj}: {obj_s} and {subj}: {subj_s}, relation: {rel}\")\n",
    "    obj_prompt = (\n",
    "                    f\"Is the entity '{obj_s}' present in or related to the image 0: <image0>{replace_token}? \"\n",
    "                    \"Respond with 'Yes' or 'No' only.\"\n",
    "                )\n",
    "    subj_prompt = (\n",
    "                    f\"Is the entity '{subj_s}' present in or related to the image 0: <image0>{replace_token}? \"\n",
    "                    \"Respond with 'Yes' or 'No' only.\"\n",
    "                )\n",
    "    context_prompt = (\n",
    "                        f\"Analyze this image and check if it contains representations of both '{obj_s}' and '{subj_s}' from the knowledge graph. \"\n",
    "                        f\"Explain how '{obj_s}' and '{subj_s}' are depicted in the image or confirm if they are related to the content of the image.\"\n",
    "                        \"Respond in one sentence.\"\n",
    "                    )\n",
    "    prompt_list = [obj_prompt, subj_prompt, context_prompt]\n",
    "    \n",
    "    # loop through images, when image path exists\n",
    "    related_cnt = 0\n",
    "    obj_context_list, subj_context_list = [], []\n",
    "    \n",
    "    # obj\n",
    "    if os.path.exists(obj_path): # and obj == '/m/01sl1q':\n",
    "        print(f\"\\t======== Processing object {obj} images...\")\n",
    "        obj_images_list = get_useful_images(obj_path, model, processor, prompt_list, select_best_one=False)\n",
    "    else:\n",
    "        print(f\"\\t======== Object {obj} images don't exist\")\n",
    "    \n",
    "    # subj\n",
    "    if os.path.exists(subj_path):\n",
    "        print(f\"\\t======== Processing subject {subj} images...\")\n",
    "        subj_images_list = get_useful_images(subj_path, model, processor, prompt_list, select_best_one=False)\n",
    "    else:\n",
    "        print(f\"\\t======== Subject {subj} images don't exist\")\n",
    "\n",
    "    all_useful_images = obj_images_list + subj_images_list\n",
    "    \n",
    "    # Ignore this triple, if no useful images are found\n",
    "    if len(all_useful_images) == 0:\n",
    "        print(f\"Triple {triple_cnt} skipped!\")\n",
    "        print(\"========\\n\")\n",
    "        continue\n",
    "    \n",
    "    # Prompt\n",
    "    # black_image_path = \"/blue/daisyw/dkasinets/mmkgc/LLM-MMKGC/context_generation/black_image.jpg\"\n",
    "    # with open(black_image_path) as f:\n",
    "    #     black_image = Image.open(black_image_path)\n",
    "    # only_black_image = [black_image]\n",
    "            \n",
    "    # all_useful_images\n",
    "    summary_prompt = get_image_relation_prompt(all_useful_images, obj_s, subj_s)\n",
    "    print(\"prompt: \")\n",
    "    print(summary_prompt)\n",
    "    \n",
    "    # all_useful_images\n",
    "    triple_context_list = get_summary_context(all_useful_images, summary_prompt)\n",
    "    \n",
    "    triple_context_cnt = len(triple_context_list)\n",
    "    print(f\">>>> Related images found: {triple_context_cnt}\")\n",
    "    \n",
    "    # save generations to dictionary\n",
    "    if triple_context_cnt > 1:\n",
    "        ent_context_dict[obj][\"context_dict\"][subj] += triple_context_list\n",
    "        ent_context_dict[subj][\"context_dict\"][obj] += triple_context_list\n",
    "\n",
    "    # save to json file\n",
    "    # with open(os.path.join(context_json_dir, \"t5-train_context-summary-v1.json\"), 'w') as out_f:\n",
    "    #     js.dump(ent_context_dict, out_f, indent=4)\n",
    "        \n",
    "    print(f\"Triple {triple_cnt} processed...\")\n",
    "    print(\"========\\n\")\n",
    "    \n",
    "    # TODO: TESTING. Temporary break!\n",
    "    if idx == 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55758b-40cb-4ee3-98ef-2afc49966d60",
   "metadata": {},
   "source": [
    "### Setting_3: summary for head entity with its images only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7f990-7709-4b49-b9de-a0de7623a7df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "data_folder = \"/blue/daisyw/ma.haodi/LLM-MMKGC/data/fb15k-237/\"\n",
    "train_triples_file = \"train.txt\"\n",
    "\n",
    "# triples\n",
    "train_triples = []\n",
    "train_ent_dict = collections.defaultdict(list)\n",
    "with open(os.path.join(data_folder, train_triples_file)) as triple_f:\n",
    "    for line in triple_f.readlines():\n",
    "        obj, rel, subj = line.strip().split(\"\\t\")\n",
    "        train_triples.append([obj, rel, subj])\n",
    "        train_ent_dict[obj].append([rel, subj])\n",
    "        train_ent_dict[subj].append([rel, obj])\n",
    "        \n",
    "\n",
    "# ent id 2 text\n",
    "# load id2txt into dictionary\n",
    "ent2str_file = \"entity_strings.del\"\n",
    "ent2str_dict = {}\n",
    "with open(os.path.join(data_folder, ent2str_file)) as e2s_f:\n",
    "    for line in e2s_f.readlines():\n",
    "        e_id, e_str = line.strip().split(\"\\t\")\n",
    "        ent2str_dict[e_id] = e_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b1a68-7917-4cbe-b782-ff53307f6c82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate summary for object\n",
    "def ent_images_summary(ent_path, ent_str, model, processor, prompt_list, device, model_mod=\"single\"):\n",
    "    summary_prompt = prompt_list[0]\n",
    "    # collect all images\n",
    "    ent_images = []\n",
    "    for image_path in os.listdir(ent_path):\n",
    "        yes_probs, no_probs = [], []\n",
    "        image_full_path = os.path.join(ent_path, image_path)\n",
    "        with open(image_full_path) as f:\n",
    "            # print(f\"Content of '{image_full_path}'\")\n",
    "            try:\n",
    "                image = Image.open(image_full_path).convert(\"RGB\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during opening image {image_full_path}:\", e)\n",
    "                continue\n",
    "            # resize image if its size is 1, 1\n",
    "            if image.size == (1, 1) or image.size[0] == 1 or image.size[1] == 1:\n",
    "                print(\"Resizing the image, original size is 1*n or n*1, will cause encoder issue...\")\n",
    "                image = image.resize((10, 10))\n",
    "        ent_images.append(image)\n",
    "    if len(ent_images) == 0:\n",
    "        print(f\"No valid images for object: {ent_str}...skip...\")\n",
    "        return ''\n",
    "    ent_images = ent_images[:5]\n",
    "    \n",
    "    token_replace_str = \" \".join(\n",
    "        [\n",
    "            f\"image {idx}: <image{idx}>{replace_token},\"\n",
    "            for idx in range(len(ent_images))\n",
    "        ]\n",
    "    )\n",
    "    # remove the last ','\n",
    "    token_replace_str = token_replace_str[:-1].strip()\n",
    "    summary_prompt = summary_prompt.format(token_replace_str=token_replace_str, ent_str=ent_str)\n",
    "\n",
    "    if model_mod == 'single':  # only use the first image if using single-image model\n",
    "        context_inputs = processor(\n",
    "            images=ent_images[0], text=summary_prompt, return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            context_outputs = model.generate(\n",
    "                **context_inputs,\n",
    "                do_sample=True,  # generate with sample may return conflict with prob\n",
    "                # num_beams=5,\n",
    "                max_length=1024,\n",
    "                min_length=1,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.5,\n",
    "                length_penalty=1.0,\n",
    "                temperature=1,  # generate with greedy or not\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "    elif model_mod == 'multi':\n",
    "        context_inputs = processor(images=ent_images, text=summary_prompt, return_tensors=\"pt\")\n",
    "        context_inputs['pixel_values'] = context_inputs['pixel_values'].to(torch.bfloat16)\n",
    "        print(context_inputs['pixel_values'].shape)\n",
    "        context_inputs['img_mask'] = torch.tensor([[1 for i in range(len(ent_images))]])\n",
    "        context_inputs['pixel_values'] = context_inputs['pixel_values'].unsqueeze(0)\n",
    "        print(context_inputs['pixel_values'].shape)\n",
    "\n",
    "        context_inputs = context_inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            context_outputs = model.generate(\n",
    "                pixel_values=context_inputs['pixel_values'],\n",
    "                input_ids=context_inputs['input_ids'],\n",
    "                attention_mask=context_inputs['attention_mask'],\n",
    "                img_mask=context_inputs['img_mask'],\n",
    "                do_sample=False,\n",
    "                max_length=50,\n",
    "                min_length=1,\n",
    "                set_min_padding_size=False,\n",
    "            )\n",
    "\n",
    "    context_text = processor.batch_decode(\n",
    "        context_outputs, skip_special_tokens=True\n",
    "    )[0].strip()\n",
    "\n",
    "    print(f'\\t>>>> Generated context: {context_text}')\n",
    "    return context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98bfda3-64fb-41c1-82cd-cef4fbec961d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from pathlib import Path\n",
    "import json as js\n",
    "\n",
    "# for each triple, we generate the caption based on the entity\n",
    "# the idea is to select images that are about the triple\n",
    "triple_cnt = 0\n",
    "resume = 0\n",
    "stop = -1\n",
    "\n",
    "if stop == -1:\n",
    "    stop = len(train_triples)\n",
    "\n",
    "# save generations to a dicitonary\n",
    "ent_context_dict = dict()\n",
    "# load from file if previously exist\n",
    "context_folder = os.path.join(kg_img_context_folder, \"context\")\n",
    "# Path(context_folder).mkdir(parents=True, exist_ok=True)\n",
    "# if os.path.exists(os.path.join(context_folder, \"t5-train_context-v1.json\")):\n",
    "#     with open(os.path.join(context_folder, \"t5-train_context-v1.json\")) as context_f:\n",
    "#         ent_context_dict = js.load(context_f)\n",
    "\n",
    "total_cnt = len(train_triples)\n",
    "print(total_cnt)\n",
    "for idx, (obj, rel, subj) in enumerate(train_triples):\n",
    "    if idx < resume:\n",
    "        continue\n",
    "    if stop != -1 and idx > stop:\n",
    "        break\n",
    "\n",
    "    triple_cnt += 1\n",
    "    # if triple_cnt > 1:\n",
    "    #     break\n",
    "\n",
    "    obj_s, subj_s = ent2str_dict[obj], ent2str_dict[subj]\n",
    "    obj_path = os.path.join(image_folder, obj[1:].replace('/', '.'))\n",
    "    subj_path = os.path.join(image_folder, subj[1:].replace('/', '.'))\n",
    "    print(\"========\")\n",
    "    # skip if we already processed this triple\n",
    "    if (obj in ent_context_dict):\n",
    "        print(f\"Triples between {obj}:{obj_s} and {subj}:{subj_s} already processed... Skip...\")\n",
    "        print(\"========\\n\")\n",
    "        continue\n",
    "        \n",
    "    # initialize context dictionary for obj and subj\n",
    "    if obj not in ent_context_dict:\n",
    "        ent_context_dict[obj] = dict()\n",
    "        ent_context_dict[obj][\"ent_str\"] = obj_s\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    print(f\"Generation about {obj}: {obj_s} and {subj}: {subj_s}, relation: {rel}\")\n",
    "    obj_summary_prompt = (\n",
    "        \"Analyze the set of images {token_replace_str}\"\n",
    "        \"The model should primarily rely on the visual information in the images to understand the context.\"\n",
    "        \"In one complete, longer sentence, summarize what these images show about {ent_str}\"\n",
    "        \"Don't provide trivial information. Only return useful information from the set of images.\"\n",
    "    )\n",
    "    prompt_list = [obj_summary_prompt]\n",
    "\n",
    "    obj_summary_context = ent_images_summary(\n",
    "        ent_path=obj_path, ent_str=obj_s,\n",
    "        model=model, processor=processor,\n",
    "        prompt_list=prompt_list,\n",
    "        device=device, model_mod=\"multi\"\n",
    "    )\n",
    "    if obj_summary_context is not None:\n",
    "        ent_context_dict[obj] = obj_summary_context\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # save to json file\n",
    "    # with open(os.path.join(context_folder, \"t5-train_context-setting_3-v1.json\"), 'w') as out_f:\n",
    "    #     js.dump(ent_context_dict, out_f, indent=4)\n",
    "        \n",
    "    print(f\"Triple {triple_cnt} (total {idx}/{total_cnt}) processed...\")\n",
    "    print(\"========\\n\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gDINO",
   "language": "python",
   "name": "gdino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
