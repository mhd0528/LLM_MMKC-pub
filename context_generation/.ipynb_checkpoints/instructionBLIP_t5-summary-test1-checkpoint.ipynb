{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184d5ce6-8c15-442b-9cfb-874177a3318d",
   "metadata": {},
   "source": [
    "#### Test InstructBLIP_t5 model on one entity and triple generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a59a01-cf2b-4648-919b-7a92d818dbe2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Blip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# from transformers import (\n",
    "#     InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "# )\n",
    "import torch\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "print(\"Init Blip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80cb991-9cf2-4013-8fc8-4fc6aeb4bfdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Blip 2\n"
     ]
    }
   ],
   "source": [
    "# load model that can take multiple images\n",
    "from model.instructblip import (\n",
    "    InstructBlipConfig, InstructBlipModel, InstructBlipPreTrainedModel,InstructBlipForConditionalGeneration,InstructBlipProcessor\n",
    ")\n",
    "print(\"Init Blip 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67226afc-74a8-4d2f-9477-20193096927a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:26<00:00, 28.90s/it]\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/blue/daisyw/dkasinets/envs/cache/models--BleachNick--MMICL-Instructblip-T5-xxl/.no_exist/ed4ddb6c60ff260c3c03ff149b7e91ce3496690e/generation_config.json'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "InstructBlipForConditionalGeneration(\n",
       "  (vision_model): InstructBlipVisionModel(\n",
       "    (embeddings): InstructBlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (encoder): InstructBlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-38): 39 x InstructBlipEncoderLayer(\n",
       "          (self_attn): InstructBlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): InstructBlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (qformer): InstructBlipQFormerModel(\n",
       "    (embeddings): InstructBlipQFormerEmbeddings(\n",
       "      (word_embeddings): Embedding(30523, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): InstructBlipQFormerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (language_projection): Linear(in_features=768, out_features=4096, bias=True)\n",
       "  (language_model): T5ForConditionalGeneration(\n",
       "    (shared): Embedding(32128, 4096)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 4096)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 64)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): GELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-23): 23 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): GELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 4096)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 64)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): GELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-23): 23 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): GELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32128, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CACHE_DIR = '/blue/daisyw/dkasinets/envs/cache/'\n",
    "\n",
    "# model = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "#     \"Salesforce/instructblip-flan-t5-xxl\",\n",
    "#     cache_dir=CACHE_DIR,  # Specify the new cache directory here\n",
    "#     torch_dtype=torch.float16 # new optimization\n",
    "#     )\n",
    "# processor = InstructBlipProcessor.from_pretrained(\n",
    "#     \"Salesforce/instructblip-flan-t5-xxl\",\n",
    "#     cache_dir=CACHE_DIR  # Specify the new cache directory here\n",
    "#     )\n",
    "\n",
    "model_type=\"instructblip\"\n",
    "model_ckpt=\"BleachNick/MMICL-Instructblip-T5-xxl\"\n",
    "processor_ckpt = \"Salesforce/instructblip-flan-t5-xxl\"\n",
    "\n",
    "config = InstructBlipConfig.from_pretrained(\n",
    "    model_ckpt, \n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "\n",
    "model = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "    model_ckpt,\n",
    "    config=config,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "image_palceholder = \"图\"\n",
    "sp = [image_palceholder]+[f\"<image{i}>\" for i in range(20)]\n",
    "\n",
    "processor = InstructBlipProcessor.from_pretrained(\n",
    "    processor_ckpt,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "sp = sp + processor.tokenizer.additional_special_tokens[len(sp):]\n",
    "processor.tokenizer.add_special_tokens({'additional_special_tokens':sp})\n",
    "if model.qformer.embeddings.word_embeddings.weight.shape[0] != len(processor.qformer_tokenizer):\n",
    "    model.qformer.resize_token_embeddings(len(processor.qformer_tokenizer))\n",
    "replace_token=\"\".join(32*[image_palceholder])\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ede6d31e-3f4c-4194-8cea-67627bdd2357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_palceholder=\"图\"\n",
    "sp = [image_palceholder]+[f\"<image{i}>\" for i in range(20)]\n",
    "sp = sp+processor.tokenizer.additional_special_tokens[len(sp):]\n",
    "processor.tokenizer.add_special_tokens({'additional_special_tokens':sp})\n",
    "if model.qformer.embeddings.word_embeddings.weight.shape[0] != len(processor.qformer_tokenizer):\n",
    "    model.qformer.resize_token_embeddings(len(processor.qformer_tokenizer))\n",
    "replace_token=\"\".join(32*[image_palceholder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "532b9ac8-1d08-4c83-952c-45a60b8c64a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images location in Haodi's dir\n"
     ]
    }
   ],
   "source": [
    "kg_img_context_folder = \"/orange/daisyw/ma.haodi/LLM-MMKGC/data/FB15k-237/\"\n",
    "image_folder = os.path.join(kg_img_context_folder, \"image-graph_images\")\n",
    "print(\"Images location in Haodi's dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13723d66-0242-4c50-924d-ec59e0f03292",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in InstructBLIP should be done in processing. Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your InstructBLIP model. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50)\n",
      "Set inputs\n"
     ]
    }
   ],
   "source": [
    "# image size error\n",
    "# Just for testing - Can ignore\n",
    "image_full_path = os.path.join(image_folder, \"m.0d1tm\", \"google_16.jpg\")\n",
    "image = Image.open(image_full_path).convert(\"RGB\").resize((1, 50))\n",
    "print(image.size)\n",
    "\n",
    "inputs = processor(\n",
    "        images=image, text=\"describe the image\", return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "print(\"Set inputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d1f146-5d36-41fc-8559-e302cf73d34c",
   "metadata": {},
   "source": [
    "### MMICL-Instructblip-T5 multi image test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e2c73-8f94-4d99-bc3c-2353ad33528a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ent_folder = image_full_path = os.path.join(image_folder, \"m.0d1tm\")\n",
    "# image = Image.open(os.path.join(ent_folder, \"google_1.jpg\"))\n",
    "# image1 = Image.open(os.path.join(ent_folder, \"google_12.jpg\"))\n",
    "# image2 = Image.open(os.path.join(ent_folder, \"google_23.jpg\"))\n",
    "\n",
    "image = Image.open(\"../img/cal_num1.png\")\n",
    "image1 = Image.open(\"../img/cal_num2.png\")\n",
    "image2 = Image.open(\"../img/cal_num3.png\")\n",
    "images = [image, image1, image2]\n",
    "\n",
    "prompt = [f'Use the image 0: <image0>{replace_token},image 1: <image1>{replace_token} and image 2: <image2>{replace_token} as a visual aid to help you calculate the equation accurately. image 0 is 2+1=3.\\nimage 1 is 5+6=11.\\nimage 2 is\"']\n",
    "prompt = \" \".join(prompt)\n",
    "\n",
    "inputs = processor(images=images, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "# inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "print(inputs['img_mask'].size())\n",
    "\n",
    "inputs = inputs.to(device)\n",
    "# convert input_ids to tensor\n",
    "print(type(inputs['input_ids'][0]), type(inputs['pixel_values'][0]), type(inputs['img_mask'][0]), type(inputs['attention_mask'][0]))\n",
    "\n",
    "print(torch.ones_like(inputs['input_ids']))\n",
    "print(inputs['input_ids'].device, model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "        pixel_values = inputs['pixel_values'],\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        img_mask = inputs['img_mask'],\n",
    "        do_sample=False,\n",
    "        max_length=50,\n",
    "        min_length=1,\n",
    "        set_min_padding_size =False,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs.sequences, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)\n",
    "# output: 3x6=18\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e968e33f-d9d3-4beb-91c3-07d792d09696",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### generation for single entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76dd9db2-d363-409e-86a4-d38b3a284100",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2str_dict\n"
     ]
    }
   ],
   "source": [
    "# load id2str dictionary\n",
    "# id2str_file = \"/blue/daisyw/ma.haodi/LLM-MMKGC/data/fb15k-237/entity_strings.del\"\n",
    "id2str_file = \"/blue/daisyw/dkasinets/mmkgc/LLM-MMKGC/data/fb15k-237/entity_strings.del\"\n",
    "id2str_dict = {}\n",
    "with open(id2str_file) as f:\n",
    "    for line in f.readlines():\n",
    "        e_id, e_str = line.strip().split('\\t')\n",
    "        id2str_dict[e_id] = e_str\n",
    "\n",
    "print(\"id2str_dict\") # {'/m/0hvbj': \"'N Sync\", '/m/047msdk': '(500) Days of Summer', ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac827288-0fe4-45a2-9ab7-b1513a178452",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Generating captions for ENT 'N Sync, /m/0hvbj\n",
      ">>> Generating captions for ENT (500) Days of Summer, /m/047msdk\n",
      ">>> Generating captions for ENT 1. FC Kaiserslautern, /m/01nd2c\n",
      ">>> Generating captions for ENT 1. FC Magdeburg, /m/05ls3r\n",
      ">>> Generating captions for ENT 1. FC Nürnberg, /m/07s8qm7\n",
      ">>> Generating captions for ENT 1. FC Union Berlin, /m/0264v8r\n",
      ">>> Generating captions for ENT 1. FSV Mainz 05, /m/051qvn\n",
      ">>> Generating captions for ENT 100th United States Congress, /m/02gkzs\n",
      ">>> Generating captions for ENT 101st United States Congress, /m/02cg7g\n",
      ">>> Generating captions for ENT 102nd United States Congress, /m/02bqn1\n",
      ">>> Generating captions for ENT 103rd United States Congress, /m/02bqmq\n",
      ">>> Generating captions for ENT 104th United States Congress, /m/02bqm0\n",
      ">>> Generating captions for ENT 105th United States Congress, /m/02bp37\n",
      ">>> Generating captions for ENT 106th United States Congress, /m/02bn_p\n",
      ">>> Generating captions for ENT 107th United States Congress, /m/024tkd\n",
      ">>> Generating captions for ENT 108th United States Congress, /m/024tcq\n",
      ">>> Generating captions for ENT 109th United States Congress, /m/07p__7\n",
      ">>> Generating captions for ENT 10th Screen Actors Guild Awards, /m/092c5f\n",
      ">>> Generating captions for ENT 10th United States Congress, /m/01gsrl\n",
      ">>> Generating captions for ENT 110th United States Congress, /m/06f0dc\n",
      ">>> Generating captions for ENT 111th United States Congress, /m/070m6c\n",
      ">>> Generating captions for ENT 112th United States Congress, /m/070mff\n",
      ">>> Generating captions for ENT 11th Satellite Awards, /m/0275n3y\n",
      ">>> Generating captions for ENT 11th United States Congress, /m/01gsry\n",
      ">>> Generating captions for ENT 127 Hours, /m/09gb_4p\n",
      ">>> Generating captions for ENT 12th Satellite Awards, /m/03gwpw2\n",
      ">>> Generating captions for ENT 12th Screen Actors Guild Awards, /m/09qvms\n",
      ">>> Generating captions for ENT 13 Assassins, /m/0bmc4cm\n",
      ">>> Generating captions for ENT 13 Going on 30, /m/02vzpb\n",
      ">>> Generating captions for ENT 13th Academy Awards, /m/0fy59t\n",
      ">>> Generating captions for ENT 13th Satellite Awards, /m/04n2r9h\n",
      ">>> Generating captions for ENT 13th Screen Actors Guild Awards, /m/027hjff\n",
      ">>> Generating captions for ENT 13th United States Congress, /m/01gssm\n",
      ">>> Generating captions for ENT 14th Academy Awards, /m/0fz0c2\n",
      ">>> Generating captions for ENT 14th Satellite Awards, /m/09gkdln\n",
      ">>> Generating captions for ENT 14th Screen Actors Guild Awards, /m/03gyp30\n",
      ">>> Generating captions for ENT 14th United States Congress, /m/01gssz\n",
      ">>> Generating captions for ENT 15 Minutes, /m/0992d9\n",
      ">>> Generating captions for ENT 15th Critics' Choice Awards, /m/09k5jh7\n",
      ">>> Generating captions for ENT 15th Satellite Awards, /m/0fqpc7d\n",
      ">>> Generating captions for ENT 15th Screen Actors Guild Awards, /m/058m5m4\n",
      ">>> Generating captions for ENT 15th United States Congress, /m/01gst9\n",
      ">>> Generating captions for ENT 16 Blocks, /m/06tpmy\n",
      ">>> Generating captions for ENT 16th Academy Awards, /m/0fz20l\n",
      ">>> Generating captions for ENT 16th Screen Actors Guild Awards, /m/09g90vz\n",
      ">>> Generating captions for ENT 16th United States Congress, /m/01gstn\n",
      ">>> Generating captions for ENT 17th Critics' Choice Awards, /m/0hr6lkl\n",
      ">>> Generating captions for ENT 17th Screen Actors Guild Awards, /m/0g55tzk\n",
      ">>> Generating captions for ENT 17th United States Congress, /m/01gst_\n",
      ">>> Generating captions for ENT 1896 Summer Olympics, /m/0c_tl\n",
      ">>> Generating captions for ENT 18th Screen Actors Guild Awards, /m/0hr3c8y\n",
      ">>> Generating captions for ENT 18th United States Congress, /m/01gsvb\n",
      ">>> Generating captions for ENT 1900 Summer Olympics, /m/016r9z\n",
      ">>> Generating captions for ENT 1904 Summer Olympics, /m/018wrk\n",
      ">>> Generating captions for ENT 1908 Major League Baseball season, /m/03c6s24\n",
      ">>> Generating captions for ENT 1908 Summer Olympics, /m/018qb4\n",
      ">>> Generating captions for ENT 1909 Major League Baseball season, /m/03c74_8\n",
      ">>> Generating captions for ENT 1912 Summer Olympics, /m/018ljb\n",
      ">>> Generating captions for ENT 1920 Summer Olympics, /m/0sxrz\n",
      ">>> Generating captions for ENT 1924 Summer Olympics, /m/0nbjq\n",
      ">>> Generating captions for ENT 1927 Major League Baseball Season, /m/02h7s73\n",
      ">>> Generating captions for ENT 1928 Summer Olympics, /m/0lv1x\n",
      ">>> Generating captions for ENT 1932 Summer Olympics, /m/0lk8j\n",
      ">>> Generating captions for ENT 1936 Summer Olympics, /m/09x3r\n",
      ">>> Generating captions for ENT 1941, /m/04t6fk\n",
      ">>> Generating captions for ENT 1948 Summer Olympics, /m/0blg2\n",
      ">>> Generating captions for ENT 1948 Winter Olympics, /m/0blfl\n",
      ">>> Generating captions for ENT 1952 Summer Olympics, /m/0lgxj\n",
      ">>> Generating captions for ENT 1952 Winter Olympics, /m/0124ld\n",
      ">>> Generating captions for ENT 1954 Major League Baseball season, /m/04110b0\n",
      ">>> Generating captions for ENT 1956 Summer Olympics, /m/0ldqf\n",
      ">>> Generating captions for ENT 1960 Summer Olympics, /m/0lbd9\n",
      ">>> Generating captions for ENT 1960 Winter Olympics, /m/015pkt\n",
      ">>> Generating captions for ENT 1964 Summer Olympics, /m/0lbbj\n",
      ">>> Generating captions for ENT 1964 Winter Olympics, /m/01f1jf\n",
      ">>> Generating captions for ENT 1965 Major League Baseball Season, /m/05kcgsf\n",
      ">>> Generating captions for ENT 1968 Summer Olympics, /m/0l998\n",
      ">>> Generating captions for ENT 1968 Winter Olympics, /m/015l4k\n",
      ">>> Generating captions for ENT 1972 Summer Olympics, /m/0l98s\n",
      ">>> Generating captions for ENT 1972 Winter Olympics, /m/01f1jy\n",
      ">>> Generating captions for ENT 1976 Summer Olympics, /m/0jkvj\n",
      ">>> Generating captions for ENT 1976 Winter Olympics, /m/01f1kd\n",
      ">>> Generating captions for ENT 1980 NCAA Men's Division I Basketball Tournament, /m/0b_6h7\n",
      ">>> Generating captions for ENT 1980 Summer Olympics, /m/0l6vl\n",
      ">>> Generating captions for ENT 1980 Winter Olympics, /m/0sx8l\n",
      ">>> Generating captions for ENT 1982 Berlin International Film Festival, /m/05ys0ws\n",
      ">>> Generating captions for ENT 1982 Cannes Film Festival, /m/05f5rsr\n",
      ">>> Generating captions for ENT 1982 Lebanon War, /m/018vbf\n",
      ">>> Generating captions for ENT 1983 NCAA Men's Division I Basketball Tournament, /m/0b_6jz\n",
      ">>> Generating captions for ENT 1984 Berlin International Film Festival, /m/05ys0wz\n",
      ">>> Generating captions for ENT 1984 NCAA Men's Division I Basketball Tournament, /m/0b_6lb\n",
      ">>> Generating captions for ENT 1984 Summer Olympics, /m/0l6ny\n",
      ">>> Generating captions for ENT 1984 Winter Olympics, /m/0sx92\n",
      ">>> Generating captions for ENT 1985 Major League Baseball season, /m/027pwzc\n",
      ">>> Generating captions for ENT 1985 NCAA Men's Division I Basketball Tournament, /m/0b_6mr\n",
      ">>> Generating captions for ENT 1986 NCAA Men's Division I Basketball Tournament, /m/0bzrsh\n",
      ">>> Generating captions for ENT 1987 NCAA Men's Division I Basketball Tournament, /m/0b_6pv\n",
      ">>> Generating captions for ENT 1988 NCAA Men's Division I Basketball Tournament, /m/0b_6q5\n",
      ">>> Generating captions for ENT 1988 Summer Olympics, /m/0l6mp\n",
      ">>> Generating captions for ENT 1988 Winter Olympics, /m/019n8z\n",
      ">>> Generating captions for ENT 1989 Berlin International Film Festival, /m/05ys0xf\n",
      ">>> Generating captions for ENT 1989 NCAA Men's Division I Basketball Tournament, /m/0b_6qj\n",
      ">>> Generating captions for ENT 1990 NCAA Men's Division I Basketball Tournament, /m/0b_6rk\n",
      ">>> Generating captions for ENT 1991 NCAA Men's Division I Basketball Tournament, /m/0b_6s7\n",
      ">>> Generating captions for ENT 1992 NCAA Men's Division I Basketball Tournament, /m/0b_6v_\n",
      ">>> Generating captions for ENT 1992 Summer Olympics, /m/0l6m5\n",
      ">>> Generating captions for ENT 1992 Winter Olympics, /m/0sx7r\n",
      ">>> Generating captions for ENT 1993 NCAA Men's Division I Basketball Tournament, /m/0b_6x2\n",
      ">>> Generating captions for ENT 1994 NCAA Men's Division I Basketball Tournament, /m/0b_6xf\n",
      ">>> Generating captions for ENT 1994 Winter Olympics, /m/0swff\n",
      ">>> Generating captions for ENT 1995 Major League Baseball Draft, /m/047dpm0\n",
      ">>> Generating captions for ENT 1995 NCAA Men's Division I Basketball Tournament, /m/0bzrxn\n",
      ">>> Generating captions for ENT 1996 NCAA Men's Division I Basketball Tournament, /m/0b_6zk\n",
      ">>> Generating captions for ENT 1996 Summer Olympics, /m/0jhn7\n",
      ">>> Generating captions for ENT 1997 Major League Baseball Draft, /m/04f4z1k\n",
      ">>> Generating captions for ENT 1997 NCAA Men's Division I Basketball Tournament, /m/0b_6_l\n",
      ">>> Generating captions for ENT 1998 NCAA Men's Division I Basketball Tournament, /m/0b_75k\n",
      ">>> Generating captions for ENT 1998 Winter Olympics, /m/0swbd\n",
      ">>> Generating captions for ENT 1999 NCAA Men's Division I Basketball Tournament, /m/0b_71r\n",
      ">>> Generating captions for ENT 19th United States Congress, /m/01gsvp\n",
      ">>> Generating captions for ENT 19th century, /m/08b3m\n",
      ">>> Generating captions for ENT 1st United States Congress, /m/01grmk\n",
      ">>> Generating captions for ENT 2 Fast 2 Furious, /m/024lff\n",
      ">>> Generating captions for ENT 2000 Cannes Film Festival, /m/03wf1p2\n",
      ">>> Generating captions for ENT 2000 Major League Baseball season, /m/026fmqm\n",
      ">>> Generating captions for ENT 2000 NCAA Men's Division I Basketball Tournament, /m/0b_72t\n",
      ">>> Generating captions for ENT 2000 Summer Olympics, /m/0jdk_\n",
      ">>> Generating captions for ENT 2001 NCAA Men's Division I Basketball Tournament, /m/0b_770\n",
      ">>> Generating captions for ENT 2001: A Space Odyssey, /m/08ct6\n",
      ">>> Generating captions for ENT 2002 Major League Baseball Draft, /m/02z6872\n",
      ">>> Generating captions for ENT 2002 NCAA Men's Division I Basketball Tournament, /m/0b_734\n",
      ">>> Generating captions for ENT 2002 Winter Olympics, /m/09n48\n",
      ">>> Generating captions for ENT 2003 Major League Baseball Draft, /m/02x2khw\n",
      ">>> Generating captions for ENT 2003 Major League Baseball season, /m/025ygws\n",
      ">>> Generating captions for ENT 2003 NCAA Men's Division I Basketball Tournament, /m/0b_77q\n",
      ">>> Generating captions for ENT 2003 NFL Draft, /m/092j54\n",
      ">>> Generating captions for ENT 2003 invasion of Iraq, /m/01cpp0\n",
      ">>> Generating captions for ENT 2004 Major League Baseball Draft, /m/02rl201\n",
      ">>> Generating captions for ENT 2004 Major League Baseball season, /m/025ygqm\n",
      ">>> Generating captions for ENT 2004 NCAA Men's Division I Basketball Tournament, /m/0b_756\n",
      ">>> Generating captions for ENT 2004 NFL Draft, /m/03nt7j\n",
      ">>> Generating captions for ENT 2004 Summer Olympics, /m/0kbvb\n",
      ">>> Generating captions for ENT 2005 Major League Baseball Draft, /m/02pq_x5\n",
      ">>> Generating captions for ENT 2005 Major League Baseball season, /m/0dx84s\n",
      ">>> Generating captions for ENT 2005 NCAA Men's Division I Basketball Tournament, /m/05g_nr\n",
      ">>> Generating captions for ENT 2005 NFL Draft, /m/05vsb7\n",
      ">>> Generating captions for ENT 2006 Major League Baseball Draft, /m/02pq_rp\n",
      ">>> Generating captions for ENT 2006 Major League Baseball season, /m/027mvrc\n",
      ">>> Generating captions for ENT 2006 NCAA Men's Division I Basketball Tournament, /m/0bqthy\n",
      ">>> Generating captions for ENT 2006 NFL Draft, /m/09l0x9\n",
      ">>> Generating captions for ENT 2006 Winter Olympics, /m/0kbvv\n",
      ">>> Generating captions for ENT 2007 Major League Baseball Draft, /m/02r6gw6\n",
      ">>> Generating captions for ENT 2007 Major League Baseball season, /m/0285r5d\n",
      ">>> Generating captions for ENT 2007 NCAA Men's Division I Basketball Tournament, /m/0br1xn\n",
      ">>> Generating captions for ENT 2007 NFL Draft, /m/0g3zpp\n",
      ">>> Generating captions for ENT 2008 Berlin International Film Festival, /m/0bx_f_t\n",
      ">>> Generating captions for ENT 2008 Major League Baseball season, /m/03c6sl9\n",
      ">>> Generating captions for ENT 2008 NCAA Men's Division I Basketball Tournament, /m/0br1x_\n",
      ">>> Generating captions for ENT 2008 NFL Draft, /m/02qw1zx\n",
      ">>> Generating captions for ENT 2008 NFL season, /m/03gqdq7\n",
      ">>> Generating captions for ENT 2008 Summer Olympics, /m/0kbws\n",
      ">>> Generating captions for ENT 2008 Sundance Film Festival, /m/02z6gky\n",
      ">>> Generating captions for ENT 2008 Toronto International Film Festival, /m/04grdgy\n",
      ">>> Generating captions for ENT 2008 Tour de France, /m/027yjnv\n",
      ">>> Generating captions for ENT 2009 Berlin International Film Festival, /m/059_y8d\n",
      ">>> Generating captions for ENT 2009 Major League Baseball season, /m/04n36qk\n",
      ">>> Generating captions for ENT 2009 NCAA Men's Division I Basketball Tournament, /m/0cc8q3\n",
      ">>> Generating captions for ENT 2009 NFL season, /m/04y6_qr\n",
      ">>> Generating captions for ENT 2009 Sundance Film Festival, /m/03nn7l2\n",
      ">>> Generating captions for ENT 2009 Toronto International Film Festival, /m/04_m9gk\n",
      ">>> Generating captions for ENT 2009 Tour de France, /m/02rxd26\n",
      ">>> Generating captions for ENT 2009 World Championships in Athletics, /m/09hz7t\n",
      ">>> Generating captions for ENT 2010, /m/04gcyg\n",
      ">>> Generating captions for ENT 2010 Berlin International Film Festival, /m/09rwjly\n",
      ">>> Generating captions for ENT 2010 NCAA Men's Division I Basketball Tournament, /m/0f9rw9\n",
      ">>> Generating captions for ENT 2010 Sundance Film Festival, /m/0cmd3zy\n",
      ">>> Generating captions for ENT 2010 Toronto International Film Festival, /m/0bmj62v\n",
      ">>> Generating captions for ENT 2010 Winter Olympics, /m/018ctl\n",
      ">>> Generating captions for ENT 2011 Berlin International Film Festival, /m/0g57ws5\n",
      ">>> Generating captions for ENT 2011 Sundance Film Festival, /m/0fpkxfd\n",
      ">>> Generating captions for ENT 2011 Toronto International Film Festival, /m/0gg7gsl\n",
      ">>> Generating captions for ENT 2012, /m/047vnkj\n",
      ">>> Generating captions for ENT 2012 Berlin International Film Festival, /m/0hrcs29\n",
      ">>> Generating captions for ENT 2012 British Academy Film Awards, /m/0h_cssd\n",
      ">>> Generating captions for ENT 2012 Summer Olympics, /m/06sks6\n",
      ">>> Generating captions for ENT 2012 Sundance Film Festival, /m/0hr30wt\n",
      ">>> Generating captions for ENT 2012 Toronto International Film Festival, /m/0j63cyr\n",
      ">>> Generating captions for ENT 2046, /m/01f8f7\n",
      ">>> Generating captions for ENT 20th Century Fox, /m/016tt2\n",
      ">>> Generating captions for ENT 20th Century Fox Home Entertainment, /m/05s_k6\n",
      ">>> Generating captions for ENT 20th Century Fox Television, /m/0f721s\n",
      ">>> Generating captions for ENT 20th United States Congress, /m/01gt99\n",
      ">>> Generating captions for ENT 20th century, /m/089_x\n",
      ">>> Generating captions for ENT 20th-century classical music, /m/01wqlc\n",
      ">>> Generating captions for ENT 20th-century philosophy, /m/03dft3\n",
      ">>> Generating captions for ENT 21 Grams, /m/02d478\n",
      ">>> Generating captions for ENT 21 Jump Street (Crime Fiction Film), /m/0bq8tmw\n",
      ">>> Generating captions for ENT 21st Academy Awards, /m/0fzrhn\n",
      ">>> Generating captions for ENT 23rd Academy Awards, /m/0dznvw\n",
      ">>> Generating captions for ENT 23rd United States Congress, /m/01gtbb\n",
      ">>> Generating captions for ENT 24, /m/0hz55\n",
      ">>> Generating captions for ENT 24: Redemption, /m/025ts_z\n",
      ">>> Generating captions for ENT 24th Academy Awards, /m/0d__c3\n",
      ">>> Generating captions for ENT 25th Academy Awards, /m/05hmp6\n",
      ">>> Generating captions for ENT 25th United States Congress, /m/01gtc0\n",
      ">>> Generating captions for ENT 26th Academy Awards, /m/0c53zb\n",
      ">>> Generating captions for ENT 26th United States Congress, /m/01gtcc\n",
      ">>> Generating captions for ENT 27th Academy Awards, /m/0c53vt\n",
      ">>> Generating captions for ENT 27th United States Congress, /m/01gtcq\n",
      ">>> Generating captions for ENT 28 Days Later, /m/012kyx\n",
      ">>> Generating captions for ENT 28th Academy Awards, /m/0fv89q\n",
      ">>> Generating captions for ENT 28th Golden Globe Awards, /m/026kqs9\n",
      ">>> Generating captions for ENT 28th United States Congress, /m/01gtdd\n",
      ">>> Generating captions for ENT 29th Academy Awards, /m/0fy6bh\n",
      ">>> Generating captions for ENT 2K Games, /m/07zl6m\n",
      ">>> Generating captions for ENT 2nd United States Congress, /m/01grnp\n",
      ">>> Generating captions for ENT 3 Idiots, /m/047q2k1\n",
      ">>> Generating captions for ENT 30 Days of Night, /m/0gy30w\n",
      ">>> Generating captions for ENT 30 Rock, /m/0d68qy\n",
      ">>> Generating captions for ENT 300 (Historical Epic Film), /m/07f_t4\n",
      ">>> Generating captions for ENT 3000 Miles to Graceland, /m/035gnh\n",
      ">>> Generating captions for ENT 30th Academy Awards, /m/0fzrtf\n",
      ">>> Generating captions for ENT 30th United States Congress, /m/01h7xx\n",
      ">>> Generating captions for ENT 31st Academy Awards, /m/0fz2y7\n",
      ">>> Generating captions for ENT 31st United States Congress, /m/043djx\n",
      ">>> Generating captions for ENT 32nd Academy Awards, /m/0c6vcj\n",
      ">>> Generating captions for ENT 32nd United States Congress, /m/03rl1g\n",
      ">>> Generating captions for ENT 33rd Academy Awards, /m/0ftlxj\n",
      ">>> Generating captions for ENT 34th Academy Awards, /m/0ftlkg\n",
      ">>> Generating captions for ENT 34th Canadian Parliament, /m/03h_f4\n",
      ">>> Generating captions for ENT 35 mm film, /m/0cj16\n",
      ">>> Generating captions for ENT 35th Academy Awards, /m/0dthsy\n",
      ">>> Generating captions for ENT 35th Annual Grammy Awards, /m/01xqqp\n",
      ">>> Generating captions for ENT 35th Canadian Parliament, /m/034_7s\n",
      ">>> Generating captions for ENT 36th Academy Awards, /m/0fk0xk\n",
      ">>> Generating captions for ENT 37th Academy Awards, /m/0c4hx0\n",
      ">>> Generating captions for ENT 37th Golden Globe Awards, /m/026kq4q\n",
      ">>> Generating captions for ENT 38th Academy Awards, /m/0c4hnm\n",
      ">>> Generating captions for ENT 38th Annual Grammy Awards, /m/01mhwk\n",
      ">>> Generating captions for ENT 38th Canadian Parliament, /m/01gvxh\n",
      ">>> Generating captions for ENT 38th Daytime Emmy Awards, /m/0gkxgfq\n",
      ">>> Generating captions for ENT 38th People's Choice Awards, /m/0hhtgcw\n",
      ">>> Generating captions for ENT 39th Academy Awards, /m/0dth6b\n",
      ">>> Generating captions for ENT 39th Annual Grammy Awards, /m/01mh_q\n",
      ">>> Generating captions for ENT 39th Canadian Parliament, /m/04fhps\n",
      ">>> Generating captions for ENT 39th Daytime Emmy Awards, /m/0jt3qpk\n",
      ">>> Generating captions for ENT 3:10 to Yuma (Action/Adventure Film) #2, /m/0btbyn\n",
      ">>> Generating captions for ENT 3D Realms, /m/01skcy\n",
      ">>> Generating captions for ENT 3rd Rock from the Sun, /m/01lv85\n",
      ">>> Generating captions for ENT 3rd United States Congress, /m/01grp0\n",
      ">>> Generating captions for ENT 40th Academy Awards, /m/0c4hgj\n",
      ">>> Generating captions for ENT 40th Canadian Parliament, /m/04lgybj\n",
      ">>> Generating captions for ENT 41st Academy Awards, /m/0bzkvd\n",
      ">>> Generating captions for ENT 41st Annual Grammy Awards, /m/01c6qp\n",
      ">>> Generating captions for ENT 42nd Academy Awards, /m/0bz6l9\n",
      ">>> Generating captions for ENT 42nd Annual Grammy Awards, /m/01s695\n",
      ">>> Generating captions for ENT 43rd Academy Awards, /m/0bz6sb\n",
      ">>> Generating captions for ENT 43rd Annual Grammy Awards, /m/01bx35\n",
      ">>> Generating captions for ENT 44th Academy Awards, /m/0bzknt\n",
      ">>> Generating captions for ENT 44th Annual Grammy Awards, /m/013b2h\n",
      ">>> Generating captions for ENT 45th Academy Awards, /m/0bzkgg\n",
      ">>> Generating captions for ENT 45th Annual Grammy Awards, /m/019bk0\n",
      ">>> Generating captions for ENT 46th Academy Awards, /m/0bzk8w\n",
      ">>> Generating captions for ENT 46th Annual Grammy Awards, /m/02cg41\n",
      ">>> Generating captions for ENT 47th Academy Awards, /m/0bzk2h\n",
      ">>> Generating captions for ENT 47th Annual Grammy Awards, /m/056878\n",
      ">>> Generating captions for ENT 48th Academy Awards, /m/0bzjvm\n",
      ">>> Generating captions for ENT 48th Annual Grammy Awards, /m/09n4nb\n",
      ">>> Generating captions for ENT 49th Academy Awards, /m/0bzjgq\n",
      ">>> Generating captions for ENT 49th Annual Grammy Awards, /m/0gpjbt\n",
      ">>> Generating captions for ENT 4AD, /m/026k4d\n",
      ">>> Generating captions for ENT 4th United States Congress, /m/01grpc\n",
      ">>> Generating captions for ENT 50 Cent, /m/01vvyc_\n",
      ">>> Generating captions for ENT 50 First Dates, /m/02f6g5\n",
      ">>> Generating captions for ENT 50/50, /m/0cmdwwg\n",
      ">>> Generating captions for ENT 505 Games, /m/026wmz6\n",
      ">>> Generating captions for ENT 50th Academy Awards, /m/03tn9w\n",
      ">>> Generating captions for ENT 50th Annual Grammy Awards, /m/02rjjll\n",
      ">>> Generating captions for ENT 51st Academy Awards, /m/0bzlrh\n",
      ">>> Generating captions for ENT 51st Annual Grammy Awards, /m/0466p0j\n",
      ">>> Generating captions for ENT 52nd Annual Grammy Awards, /m/05pd94v\n",
      ">>> Generating captions for ENT 52nd Australian Film Institute Awards, /m/0h98b3k\n",
      ">>> Generating captions for ENT 52nd Primetime Emmy Awards, /m/0bx6zs\n",
      ">>> Generating captions for ENT 54th Academy Awards, /m/0bzm81\n",
      ">>> Generating captions for ENT 54th Annual Grammy Awards, /m/0gx1673\n",
      ">>> Generating captions for ENT 54th Primetime Emmy Awards, /m/0bxs_d\n",
      ">>> Generating captions for ENT 55th Academy Awards, /m/0bzmt8\n",
      ">>> Generating captions for ENT 55th Golden Globe Awards, /m/09qftb\n",
      ">>> Generating captions for ENT 55th Primetime Emmy Awards, /m/07y_p6\n",
      ">>> Generating captions for ENT 56th Academy Awards, /m/0bzm__\n",
      ">>> Generating captions for ENT 56th Golden Globe Awards, /m/09q_6t\n",
      ">>> Generating captions for ENT 56th Primetime Emmy Awards, /m/07z31v\n",
      ">>> Generating captions for ENT 57th Academy Awards, /m/0bzn6_\n",
      ">>> Generating captions for ENT 57th Golden Globe Awards, /m/09pnw5\n",
      ">>> Generating captions for ENT 57th Primetime Emmy Awards, /m/07y9ts\n",
      ">>> Generating captions for ENT 58th Academy Awards, /m/09306z\n",
      ">>> Generating captions for ENT 58th Primetime Emmy Awards, /m/0gx_st\n",
      ">>> Generating captions for ENT 59th Academy Awards, /m/0bc773\n",
      ">>> Generating captions for ENT 59th Golden Globe Awards, /m/09p3h7\n",
      ">>> Generating captions for ENT 59th Primetime Emmy Awards, /m/02q690_\n",
      ">>> Generating captions for ENT 5th United States Congress, /m/01grpq\n",
      ">>> Generating captions for ENT 60th Academy Awards, /m/073hmq\n",
      ">>> Generating captions for ENT 60th Golden Globe Awards, /m/09p30_\n",
      ">>> Generating captions for ENT 60th Primetime Emmy Awards, /m/03nnm4t\n",
      ">>> Generating captions for ENT 61st Academy Awards, /m/073hkh\n",
      ">>> Generating captions for ENT 61st Primetime Emmy Awards, /m/05c1t6z\n",
      ">>> Generating captions for ENT 62nd Academy Awards, /m/073hgx\n",
      ">>> Generating captions for ENT 62nd Golden Globe Awards, /m/09p2r9\n",
      ">>> Generating captions for ENT 63rd Academy Awards, /m/073hd1\n",
      ">>> Generating captions for ENT 63rd Golden Globe Awards, /m/09pj68\n",
      ">>> Generating captions for ENT 63rd Primetime Emmy Awards, /m/0gvstc3\n",
      ">>> Generating captions for ENT 64th Academy Awards, /m/073h9x\n",
      ">>> Generating captions for ENT 64th Golden Globe Awards, /m/0drtv8\n",
      ">>> Generating captions for ENT 64th Primetime Creative Arts Emmy Awards, /m/0lp_cd3\n",
      ">>> Generating captions for ENT 64th Primetime Emmy Awards, /m/0hn821n\n",
      ">>> Generating captions for ENT 65th Academy Awards, /m/073h5b\n",
      ">>> Generating captions for ENT 65th Golden Globe Awards, /m/02wzl1d\n",
      ">>> Generating captions for ENT 65th Tony Awards, /m/0ds460j\n",
      ">>> Generating captions for ENT 66th Academy Awards, /m/073h1t\n",
      ">>> Generating captions for ENT 66th Golden Globe Awards, /m/0418154\n",
      ">>> Generating captions for ENT 66th Tony Awards, /m/0h_9252\n",
      ">>> Generating captions for ENT 67th Academy Awards, /m/059x66\n",
      ">>> Generating captions for ENT 67th Golden Globe Awards, /m/05zksls\n",
      ">>> Generating captions for ENT 67th World Science Fiction Convention, /m/0gwdy4\n",
      ">>> Generating captions for ENT 68th Academy Awards, /m/05q7cj\n",
      ">>> Generating captions for ENT 68th Golden Globe Awards, /m/0g5b0q5\n",
      ">>> Generating captions for ENT 69th Academy Awards, /m/02yxh9\n",
      ">>> Generating captions for ENT 69th Golden Globe Awards, /m/0hndn2q\n",
      ">>> Generating captions for ENT 6th Screen Actors Guild Awards, /m/092_25\n",
      ">>> Generating captions for ENT 6th United States Congress, /m/01grq1\n",
      ">>> Generating captions for ENT 70th Academy Awards, /m/02jp5r\n",
      ">>> Generating captions for ENT 71st Academy Awards, /m/02ywhz\n",
      ">>> Generating captions for ENT 72nd Academy Awards, /m/02yw5r\n",
      ">>> Generating captions for ENT 73rd Academy Awards, /m/02yv_b\n",
      ">>> Generating captions for ENT 74th Academy Awards, /m/02yvhx\n",
      ">>> Generating captions for ENT 75th Academy Awards, /m/02hn5v\n",
      ">>> Generating captions for ENT 76th Academy Awards, /m/02glmx\n",
      ">>> Generating captions for ENT 77th Academy Awards, /m/050yyb\n"
     ]
    }
   ],
   "source": [
    "# How many entities have images\n",
    "\n",
    "# for each entity, we generate the caption based on the entity\n",
    "no_img_ent_cnt = 0\n",
    "# for name in os.listdir(image_folder):\n",
    "for e_id in id2str_dict:\n",
    "    name = e_id[1:].replace('/', '.')\n",
    "    ent_path = os.path.join(image_folder, name)\n",
    "    # if folder， extract id, then check all images inside\n",
    "    if os.path.isdir(ent_path):\n",
    "        ent_id = '/' + name.replace('.', '/')\n",
    "        ent_str = id2str_dict[ent_id]\n",
    "        print(f\">>> Generating captions for ENT {ent_str}, {ent_id}\")\n",
    "#         for image_path in os.listdir(ent_path):\n",
    "#             image_full_path = os.path.join(ent_path, image_path)\n",
    "#             with open(image_full_path) as f:\n",
    "#                 # print(f\"Content of '{image_full_path}'\")\n",
    "#                 image = Image.open(image_full_path).convert(\"RGB\")\n",
    "#                 # prompt = f\"This image is about {ent_str}. Summarize what does it show about this entity in one sentence?\"\n",
    "#                 prompt = f\"Describe the image about {ent_str} in one sentence.\"\n",
    "#                 # generate confidence of the image related to the entity\n",
    "                \n",
    "#                 inputs = processor(\n",
    "#                     images=image, text=prompt, return_tensors=\"pt\"\n",
    "#                     ).to(device)\n",
    "\n",
    "#                 outputs = model.generate(\n",
    "#                         **inputs,\n",
    "#                         do_sample=False,\n",
    "#                         num_beams=5,\n",
    "#                         max_length=256,\n",
    "#                         min_length=1,\n",
    "#                         top_p=0.9,\n",
    "#                         repetition_penalty=1.5,\n",
    "#                         length_penalty=1.0,\n",
    "#                         temperature=1,\n",
    "#                 )\n",
    "#                 generated_text = processor.batch_decode(\n",
    "#                     outputs, skip_special_tokens=True\n",
    "#                     )[0].strip()\n",
    "#                 print(f\"\\t>>>> {image_path}: {generated_text}\")\n",
    "#         # break\n",
    "    else:\n",
    "        no_img_ent_cnt += 1\n",
    "        \n",
    "len(id2str_dict), no_img_ent_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20776f4-799e-4dd3-aae1-74a69974b9c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### generation for triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91ae209b-bfbc-46e9-be1c-d246804818c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set triples and ent2str_dict\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# data_folder = \"/blue/daisyw/ma.haodi/LLM-MMKGC/data/fb15k-237/\"\n",
    "data_folder = \"/blue/daisyw/ma.haodi/LLM-MMKGC/data/fb15k-237/\"\n",
    "train_triples_file = \"train.txt\"\n",
    "\n",
    "# triples\n",
    "train_triples = []\n",
    "train_ent_dict = collections.defaultdict(list)\n",
    "with open(os.path.join(data_folder, train_triples_file)) as triple_f:\n",
    "    for line in triple_f.readlines():\n",
    "        obj, rel, subj = line.strip().split(\"\\t\")\n",
    "        train_triples.append([obj, rel, subj])\n",
    "        train_ent_dict[obj].append([rel, subj])\n",
    "        train_ent_dict[subj].append([rel, obj])\n",
    "        \n",
    "\n",
    "# ent id 2 text\n",
    "# load id2txt into dictionary\n",
    "ent2str_file = \"entity_strings.del\"\n",
    "ent2str_dict = {}\n",
    "with open(os.path.join(data_folder, ent2str_file)) as e2s_f:\n",
    "    for line in e2s_f.readlines():\n",
    "        e_id, e_str = line.strip().split(\"\\t\")\n",
    "        ent2str_dict[e_id] = e_str\n",
    "\n",
    "print(\"Set triples and ent2str_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "952324c0-d157-41e3-83f6-e698c2d31963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to get probabilities\n",
    "# Get model output as token (for Yes and No), return Yes and No probabilities. \n",
    "def get_yes_no_probabilities(generated_outputs):\n",
    "    # Extract logits for each generation step\n",
    "    token_scores = generated_outputs.scores  # List of logits for each step\n",
    "    \n",
    "    # Ensure that there's at least one step\n",
    "    if len(token_scores) == 0:\n",
    "        print(\"No token scores available.\")\n",
    "        return None, None, generated_sentence\n",
    "    \n",
    "    # Extract logits for the first token\n",
    "    first_step_logits = token_scores[0]  # Tensor of shape (batch_size, vocab_size)\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    first_step_probs = torch.softmax(first_step_logits, dim=-1)\n",
    "    \n",
    "    # Extract token IDs for \"Yes\", \"yes\", \"No\", \"no\"\n",
    "    yes_token_ids = [\n",
    "        processor.tokenizer.encode(\"Yes\", add_special_tokens=False)[0],\n",
    "        processor.tokenizer.encode(\"yes\", add_special_tokens=False)[0]\n",
    "    ]\n",
    "    no_token_ids = [\n",
    "        processor.tokenizer.encode(\"No\", add_special_tokens=False)[0],\n",
    "        processor.tokenizer.encode(\"no\", add_special_tokens=False)[0]\n",
    "    ]\n",
    "    \n",
    "    # Calculate combined probabilities for \"Yes\"/\"yes\" and \"No\"/\"no\"\n",
    "    yes_probability = sum(first_step_probs[0, token_id].item() for token_id in yes_token_ids)\n",
    "    no_probability = sum(first_step_probs[0, token_id].item() for token_id in no_token_ids)\n",
    "    \n",
    "    return yes_probability, no_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d237ea9b-d831-4110-bf15-83627cee37db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prob_with_sentence(processor, model, prompt, image):\n",
    "    # generate confidence of the image related to the entity\n",
    "#     inputs = processor(images=[image], text=prompt, return_tensors=\"pt\")\n",
    "#     inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "#     inputs['img_mask'] = torch.tensor([[1 for i in range(len([image]))]])\n",
    "#     inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "    \n",
    "#     print(f\"inputs['pixel_values'] shape: {inputs['pixel_values'].size()}: {inputs['pixel_values']}\")\n",
    "#     print(f\"inputs['input_ids'] shape: {inputs['input_ids'].size()}: {inputs['input_ids']}\")\n",
    "#     print(f\"inputs['attention_mask'] shape: {inputs['attention_mask'].size()}: {inputs['attention_mask']}\")\n",
    "#     print(f\"inputs['img_mask'] shape: {inputs['img_mask'].size()}: {inputs['img_mask']}\")\n",
    "    \n",
    "#     inputs = inputs.to('cuda:0')\n",
    "    \n",
    "    # with torch.no_grad():\n",
    "    #     outputs = model.generate(\n",
    "    #         pixel_values = inputs['pixel_values'],\n",
    "    #         input_ids = inputs['input_ids'],\n",
    "    #         attention_mask = inputs['attention_mask'],\n",
    "    #         img_mask = inputs['img_mask'],\n",
    "    #         do_sample=False,\n",
    "    #         max_length=256,\n",
    "    #         min_length=1,\n",
    "    #         set_min_padding_size =False,\n",
    "    #         repetition_penalty=1.5,\n",
    "    #         length_penalty=1.0,\n",
    "    #         output_scores=True,\n",
    "    #         return_dict_in_generate=True,\n",
    "    #         sp_token = 32110\n",
    "    #     )\n",
    "    \n",
    "    inputs = processor(images=[image], text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "    # inputs['img_mask'] = torch.tensor([[1 for i in range(len(images))]])\n",
    "    inputs['img_mask'] = torch.tensor([[1 for i in range(len([image]))]])\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "    print(inputs['img_mask'].size())\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    # convert input_ids to tensor\n",
    "    print(type(inputs['input_ids'][0]), type(inputs['pixel_values'][0]), type(inputs['img_mask'][0]), type(inputs['attention_mask'][0]))\n",
    "\n",
    "    print(inputs['input_ids'].device, model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "                pixel_values = inputs['pixel_values'],\n",
    "                input_ids = inputs['input_ids'],\n",
    "                attention_mask = inputs['attention_mask'],\n",
    "                img_mask = inputs['img_mask'],\n",
    "                do_sample=False,\n",
    "                max_length=50,\n",
    "                min_length=1,\n",
    "                set_min_padding_size =False,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "    # extract text and yes/no probability\n",
    "    generated_text = processor.batch_decode(\n",
    "        outputs.sequences, skip_special_tokens=True\n",
    "        )[0].strip()\n",
    "    \n",
    "    # use yes/no probability as confidence\n",
    "    yes_prob, no_prob = get_yes_no_probabilities(outputs)\n",
    "    # yes_prob, no_prob = get_yes_no_cumulate_probabilities(outputs, processor, max_tokens_to_consider=1)\n",
    "    \n",
    "    return yes_prob, no_prob, generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48df6987-7ec6-497a-8c6c-9dfb593387a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def related_ent_img_search(ent_path, model, processor, prompt_list):\n",
    "    context_list = []\n",
    "    prompt_1, prompt_2, context_prompt = prompt_list\n",
    "    # print(prompt_1, prompt_2, context_prompt)\n",
    "    for image_path in os.listdir(ent_path):\n",
    "        yes_probs, no_probs = [], []\n",
    "        image_full_path = os.path.join(ent_path, image_path)\n",
    "        with open(image_full_path) as f:\n",
    "            # print(f\"Content of '{image_full_path}'\")\n",
    "            image = Image.open(image_full_path).convert(\"RGB\")\n",
    "            # resize image if its size is 1, 1\n",
    "            if image.size == (1, 1):\n",
    "                print(\"\\t>>>>Resizing the image, original size is 1*1, will cause encoder issue...\")\n",
    "                image = image.resize((10, 10))\n",
    "\n",
    "            # generation and compute prob for obj\n",
    "            yes_prob, no_prob, generated_text = get_prob_with_sentence(processor, model, prompt_1, image)\n",
    "            yes_probs.append(yes_prob)\n",
    "            no_probs.append(no_prob)\n",
    "            # log generated info\n",
    "            # print(f\"\\t>>>> Identify ent1: {generated_text}\")\n",
    "\n",
    "            # generation and compute prob for sub\n",
    "            yes_prob, no_prob, generated_text = get_prob_with_sentence(processor, model, prompt_2, image)\n",
    "            yes_probs.append(yes_prob)\n",
    "            no_probs.append(no_prob)\n",
    "            # print(f\"\\t>>>> Identify ent2: {generated_text}\")\n",
    "\n",
    "            # print(f\"\\t>>>> Image: {image_path} | Yes: {yes_prob:.6f}, No: {no_prob:.6f}\")\n",
    "            # print(f\"\\t>>>> Image: {image_path} | Yes: {[round(yes_prob, 6) for yes_prob in yes_probs]}, No: {[round(no_prob, 6) for no_prob in no_probs]}\")\n",
    "\n",
    "            # generate additional context if both entity exists\n",
    "            if yes_probs[0] > no_probs[0] and yes_probs[1] > no_probs[1]:\n",
    "                context_inputs = processor(\n",
    "                    images=image, text=context_prompt, return_tensors=\"pt\"\n",
    "                    ).to(device, torch.bfloat16)\n",
    "                with torch.no_grad():\n",
    "                    context_outputs = model.generate(\n",
    "                            **context_inputs,\n",
    "                            do_sample=True,  # generate with sample may return conflict with prob\n",
    "                            # num_beams=5,\n",
    "                            max_length=1024,\n",
    "                            min_length=1,\n",
    "                            top_p=0.9,\n",
    "                            repetition_penalty=1.5,\n",
    "                            length_penalty=1.0,\n",
    "                            temperature=1,  # generate with greedy or not\n",
    "                            output_scores=True,\n",
    "                            return_dict_in_generate=True\n",
    "                    )\n",
    "\n",
    "                # extract text and yes/no probability\n",
    "                context_text = processor.batch_decode(\n",
    "                    context_outputs.sequences, skip_special_tokens=True\n",
    "                    )[0].strip()\n",
    "                context_list.append(context_text)\n",
    "                \n",
    "                print(f\"\\t>>>> Image: {image_path} | Yes: {[round(yes_prob, 6) for yes_prob in yes_probs]}, No: {[round(no_prob, 6) for no_prob in no_probs]}\")\n",
    "                print(f\"\\t>>>> Generated context: {context_text}\\n\")\n",
    "    return context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00e9b547-b5aa-4270-a8f3-df892208ad57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate summaries\n",
    "def get_useful_images(ent_path, model, processor, prompt_list):\n",
    "    prompt_1, prompt_2, context_prompt = prompt_list\n",
    "        \n",
    "    useful_images = []\n",
    "    for image_path in os.listdir(ent_path):\n",
    "        yes_probs, no_probs = [], []\n",
    "        image_full_path = os.path.join(ent_path, image_path)\n",
    "        with open(image_full_path) as f:\n",
    "            \n",
    "            # image = Image.open(image_full_path).convert(\"RGB\")\n",
    "            image = Image.open(image_full_path)\n",
    "            # resize image if its size is 1, 1\n",
    "            if image.size == (1, 1):\n",
    "                print(\"\\t>>>>Resizing the image, original size is 1*1, will cause encoder issue...\")\n",
    "                image = image.resize((10, 10))\n",
    "\n",
    "            # generation and compute prob for obj\n",
    "            yes_prob, no_prob, generated_text = get_prob_with_sentence(processor, model, prompt_1, image)\n",
    "            yes_probs.append(yes_prob)\n",
    "            no_probs.append(no_prob)\n",
    "\n",
    "            # generation and compute prob for sub\n",
    "            yes_prob, no_prob, generated_text = get_prob_with_sentence(processor, model, prompt_2, image)\n",
    "            yes_probs.append(yes_prob)\n",
    "            no_probs.append(no_prob)\n",
    "\n",
    "            # generate additional context if both entity exists\n",
    "            if yes_probs[0] > no_probs[0] and yes_probs[1] > no_probs[1]:\n",
    "                useful_images.append(image)\n",
    "                print(f\"\\t>>>> Image: {image_path} | Yes: {[round(yes_prob, 6) for yes_prob in yes_probs]}, No: {[round(no_prob, 6) for no_prob in no_probs]}\")\n",
    "    \n",
    "    return useful_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fb14c8f-ce00-47e7-a51c-ac7b6dbcc9a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Summarize given set of images \n",
    "def get_summary_context(useful_images):\n",
    "    context_list = []\n",
    "    updated_context_prompt = (\n",
    "                        f\"Analyze this list of images and check if it contains representations of both '{obj_s}' and '{subj_s}' from the knowledge graph. \"\n",
    "                        f\"Explain how '{obj_s}' and '{subj_s}' are depicted in images or confirm if they are related to the content of images.\"\n",
    "                        \"Respond in one sentence.\"\n",
    "                    )\n",
    "    \n",
    "    inputs = processor(images=useful_images, text=updated_context_prompt, return_tensors=\"pt\")\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
    "    inputs['img_mask'] = torch.tensor([[1 for i in range(len(useful_images))]])\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "    \n",
    "    print(f\"inputs['pixel_values'] shape: {inputs['pixel_values'].size()}: {inputs['pixel_values']}\")\n",
    "    print(f\"inputs['input_ids'] shape: {inputs['input_ids'].size()}: {inputs['input_ids']}\")\n",
    "    print(f\"inputs['attention_mask'] shape: {inputs['attention_mask'].size()}: {inputs['attention_mask']}\")\n",
    "    print(f\"inputs['img_mask'] shape: {inputs['img_mask'].size()}: {inputs['img_mask']}\")\n",
    "    \n",
    "    inputs = inputs.to('cuda:0')\n",
    "    with torch.no_grad():\n",
    "        context_outputs = model.generate(\n",
    "                pixel_values = inputs['pixel_values'],\n",
    "                input_ids = inputs['input_ids'],\n",
    "                attention_mask = inputs['attention_mask'],\n",
    "                img_mask = inputs['img_mask'],\n",
    "                do_sample=False,\n",
    "                max_length=50,\n",
    "                min_length=1,\n",
    "                set_min_padding_size =False,\n",
    "        )\n",
    "    \n",
    "    # Now that we have useful images, generate context for a list of them\n",
    "    # context_inputs = processor(\n",
    "    #         images=inputs, text=updated_context_prompt, return_tensors=\"pt\"\n",
    "    #         ).to(device)\n",
    "    # with torch.no_grad():\n",
    "    #     context_outputs = model.generate(\n",
    "    #             **context_inputs,\n",
    "    #             do_sample=True,  # generate with sample may return conflict with prob\n",
    "    #             # num_beams=5,\n",
    "    #             max_length=1024,\n",
    "    #             min_length=1,\n",
    "    #             top_p=0.9,\n",
    "    #             repetition_penalty=1.5,\n",
    "    #             length_penalty=1.0,\n",
    "    #             temperature=1,  # generate with greedy or not\n",
    "    #             output_scores=True,\n",
    "    #             return_dict_in_generate=True\n",
    "    #     )\n",
    "    \n",
    "    # extract text and yes/no probability\n",
    "    # context_text = processor.batch_decode(\n",
    "    #     context_outputs.sequences, skip_special_tokens=True\n",
    "    #     )[0].strip()\n",
    "    \n",
    "    context_text = processor.batch_decode(\n",
    "        context_outputs, skip_special_tokens=True\n",
    "        )[0].strip()\n",
    "    \n",
    "    context_list.append(context_text)\n",
    "    print(f\"\\t>>>> Generated context: {context_text}\\n\")\n",
    "    \n",
    "    return context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45685d49-e41a-4953-baf9-d78b997cea6f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ignore (another probability function Haodi explored)\n",
    "# Function to get probabilities across the first few tokens\n",
    "def get_yes_no_cumulate_probabilities(generated_outputs, processor, max_tokens_to_consider=2):\n",
    "    # Extract logits for each generation step (token)\n",
    "    token_scores = generated_outputs.scores  # List of logits for each step\n",
    "    \n",
    "    # Ensure that there's at least one step\n",
    "    if len(token_scores) == 0:\n",
    "        print(\"No token scores available.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Tokenize \"Yes\", \"yes\", \"No\", \"no\" to get their token IDs\n",
    "    yes_token_ids = [\n",
    "        processor.tokenizer.encode(\"Yes\", add_special_tokens=False)[0],\n",
    "        processor.tokenizer.encode(\"yes\", add_special_tokens=False)[0]\n",
    "    ]\n",
    "    no_token_ids = [\n",
    "        processor.tokenizer.encode(\"No\", add_special_tokens=False)[0],\n",
    "        processor.tokenizer.encode(\"no\", add_special_tokens=False)[0]\n",
    "    ]\n",
    "    \n",
    "    # Initialize combined probabilities\n",
    "    yes_probability, no_probability = 1.0, 1.0\n",
    "\n",
    "    # Iterate over the first few token generation steps and accumulate probabilities\n",
    "    for i, logits in enumerate(token_scores):\n",
    "        if i >= max_tokens_to_consider:\n",
    "            break  # Only consider the first few tokens\n",
    "        \n",
    "        # Apply softmax to get token probabilities\n",
    "        token_probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Accumulate probabilities for \"Yes\"/\"yes\" and \"No\"/\"no\"\n",
    "        yes_probability *= sum(token_probs[0, token_id].item() for token_id in yes_token_ids)\n",
    "        no_probability *= sum(token_probs[0, token_id].item() for token_id in no_token_ids)\n",
    "\n",
    "    return yes_probability, no_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e9f25ab-be7b-42c0-9984-7fd169ed666d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "Generation about /m/027rn: Dominican_Republic and /m/06cx9: Republic, relation: /location/country/form_of_government\n",
      "\t======== Processing object /m/027rn images...\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n",
      "torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "cuda:0 cuda:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 94\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m======== Processing object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# obj_context_list = related_ent_img_search(obj_path, model, processor, prompt_list)\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     obj_images_list \u001b[38;5;241m=\u001b[39m \u001b[43mget_useful_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m======== Object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m, in \u001b[0;36mget_useful_images\u001b[0;34m(ent_path, model, processor, prompt_list)\u001b[0m\n\u001b[1;32m     21\u001b[0m no_probs\u001b[38;5;241m.\u001b[39mappend(no_prob)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# generation and compute prob for sub\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m yes_prob, no_prob, generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mget_prob_with_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m yes_probs\u001b[38;5;241m.\u001b[39mappend(yes_prob)\n\u001b[1;32m     26\u001b[0m no_probs\u001b[38;5;241m.\u001b[39mappend(no_prob)\n",
      "Cell \u001b[0;32mIn[19], line 50\u001b[0m, in \u001b[0;36mget_prob_with_sentence\u001b[0;34m(processor, model, prompt, image)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice, model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 50\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpixel_values\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimg_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mset_min_padding_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# extract text and yes/no probability\u001b[39;00m\n\u001b[1;32m     64\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m     65\u001b[0m     outputs\u001b[38;5;241m.\u001b[39msequences, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/LLM-MMKGC/context_generation/model/instructblip/modeling_instructblip.py:2156\u001b[0m, in \u001b[0;36mInstructBlipForConditionalGeneration.generate\u001b[0;34m(self, pixel_values, qformer_input_ids, qformer_attention_mask, input_ids, attention_mask, img_mask, set_min_padding_size, sp_token, **generate_kwargs)\u001b[0m\n\u001b[1;32m   2150\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask[:,:min_padding_size]\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;66;03m# concatenate query embeddings with prompt embeddings\u001b[39;00m\n\u001b[0;32m-> 2156\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/transformers/generation/utils.py:2971\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2968\u001b[0m unfinished_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2969\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001b[0;32m-> 2971\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[1;32m   2973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2974\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   2975\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2977\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "File \u001b[0;32m/blue/daisyw/ma.haodi/.conda/envs/gDINO/lib/python3.10/site-packages/transformers/generation/utils.py:2208\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device, cur_len, max_length)\u001b[0m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   2207\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2208\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We want this: Generating for triples\n",
    "\n",
    "import collections\n",
    "from pathlib import Path\n",
    "import json as js\n",
    "\n",
    "# for each triple, we generate the caption based on the entity\n",
    "# the idea is to select images that are about the triple\n",
    "triple_cnt = 0\n",
    "resume = 0\n",
    "stop = -1\n",
    "\n",
    "if stop == -1:\n",
    "    stop = len(train_triples)\n",
    "\n",
    "# save generations to a dicitonary\n",
    "ent_context_dict = dict()\n",
    "# load from file if previously exist\n",
    "context_folder = os.path.join(kg_img_context_folder, \"context\")\n",
    "# Path(context_folder).mkdir(parents=True, exist_ok=True)\n",
    "# if os.path.exists(os.path.join(context_folder, \"t5-train_context-v1.json\")):\n",
    "#     with open(os.path.join(context_folder, \"t5-train_context-v1.json\")) as context_f:\n",
    "#         ent_context_dict = js.load(context_f)\n",
    "\n",
    "for idx, (obj, rel, subj) in enumerate(train_triples):\n",
    "    if idx < resume:\n",
    "        continue\n",
    "    if stop != -1 and idx > stop:\n",
    "        break\n",
    "\n",
    "    triple_cnt += 1\n",
    "    # if triple_cnt > 1:\n",
    "    #     break\n",
    "\n",
    "    obj_s, subj_s = ent2str_dict[obj], ent2str_dict[subj]\n",
    "    obj_path = os.path.join(image_folder, obj[1:].replace('/', '.'))\n",
    "    subj_path = os.path.join(image_folder, subj[1:].replace('/', '.'))\n",
    "    print(\"========\")\n",
    "    \n",
    "    # Skip because we want only a single relation for subject and object (we don't care for same subject/object haveing multiple relations)\n",
    "    # Our task is adding captions to subject/object. \n",
    "    # skip if we already processed this triple\n",
    "    if (obj in ent_context_dict) and (subj in ent_context_dict[obj][\"context_dict\"]):\n",
    "        print(f\"Triples between {obj}:{obj_s} and {subj}:{subj_s} already processed... Skip...\")\n",
    "        print(\"========\\n\")\n",
    "        continue\n",
    "        \n",
    "    # initialize context dictionary for obj and subj\n",
    "    if obj not in ent_context_dict:\n",
    "        ent_context_dict[obj] = dict()\n",
    "        ent_context_dict[obj][\"ent_str\"] = obj_s\n",
    "        ent_context_dict[obj][\"context_dict\"] = collections.defaultdict(list)\n",
    "    if subj not in ent_context_dict[obj][\"context_dict\"]:\n",
    "        ent_context_dict[obj][\"context_dict\"][subj] = []\n",
    "    if subj not in ent_context_dict:\n",
    "        ent_context_dict[subj] = dict()\n",
    "        ent_context_dict[subj][\"ent_str\"] = subj_s\n",
    "        ent_context_dict[subj][\"context_dict\"] = collections.defaultdict(list)\n",
    "    if obj not in ent_context_dict[subj][\"context_dict\"]:\n",
    "        ent_context_dict[subj][\"context_dict\"][obj] = []\n",
    "    \n",
    "    print(f\"Generation about {obj}: {obj_s} and {subj}: {subj_s}, relation: {rel}\")\n",
    "    # prompt = f\"Does the image show the relation between {obj_s} and {subj_s}?\"\n",
    "    # obj_prompt = f\"Does the image have {obj_s} in it?\"\n",
    "    # subj_prompt = f\"Does the image have {subj_s} in it?\"\n",
    "    # obj_prompt = f\"Is {obj_s} related to the image? Return Yes/No.\"\n",
    "    # subj_prompt = f\"Is {subj_s} related to the image? return Yes/No.\"\n",
    "    # context_prompt = f\"What does the image say about {obj_s} and {subj_s}\"\n",
    "    obj_prompt = (\n",
    "                    f\"Is the entity '{obj_s}' present in or related to this image? \"\n",
    "                    \"Respond with 'Yes' or 'No' only.\"\n",
    "                )\n",
    "    subj_prompt = (\n",
    "                    f\"Is the entity '{subj_s}' present in or related to this image? \"\n",
    "                    \"Respond with 'Yes' or 'No' only.\"\n",
    "                )\n",
    "    # context_prompt = (\n",
    "    #                     f\"Analyze this image and check if it contains representations of both '{obj_s}' and '{subj_s}' from the FB15k-237 knowledge graph. \"\n",
    "    #                     f\"In one sentence, describe how both '{obj_s}' and '{subj_s}' are depicted or related to the image, and ensure that both entities are mentioned in the sentence.\"\n",
    "    #                 )\n",
    "    context_prompt = (\n",
    "                        f\"Analyze this image and check if it contains representations of both '{obj_s}' and '{subj_s}' from the knowledge graph. \"\n",
    "                        f\"Explain how '{obj_s}' and '{subj_s}' are depicted in the image or confirm if they are related to the content of the image.\"\n",
    "                        \"Respond in one sentence.\"\n",
    "                    )\n",
    "    prompt_list = [obj_prompt, subj_prompt, context_prompt]\n",
    "    \n",
    "    # loop through images, when image path exists\n",
    "    related_cnt = 0\n",
    "    obj_context_list, subj_context_list = [], []\n",
    "    if os.path.exists(obj_path): # and obj == '/m/01sl1q':\n",
    "        print(f\"\\t======== Processing object {obj} images...\")\n",
    "        # obj_context_list = related_ent_img_search(obj_path, model, processor, prompt_list)\n",
    "        obj_images_list = get_useful_images(obj_path, model, processor, prompt_list)\n",
    "    else:\n",
    "        print(f\"\\t======== Object {obj} images don't exist\")\n",
    "    if os.path.exists(subj_path):\n",
    "        print(f\"\\t======== Processing subject {subj} images...\")\n",
    "        # subj_context_list = related_ent_img_search(subj_path, model, processor, prompt_list)\n",
    "        subj_images_list = get_useful_images(subj_path, model, processor, prompt_list)\n",
    "    else:\n",
    "        print(f\"\\t======== Subject {subj} images don't exist\")\n",
    "\n",
    "    all_useful_images = obj_images_list + subj_images_list\n",
    "    # triple_context_list = obj_context_list + subj_context_list\n",
    "    triple_context_list = get_summary_context(all_useful_images)\n",
    "    \n",
    "    triple_context_cnt = len(triple_context_list)\n",
    "    print(f\">>>> Related images found: {triple_context_cnt}\")\n",
    "    \n",
    "    # save generations to dictionary\n",
    "    if triple_context_cnt > 1:\n",
    "        ent_context_dict[obj][\"context_dict\"][subj] += triple_context_list\n",
    "        ent_context_dict[subj][\"context_dict\"][obj] += triple_context_list\n",
    "\n",
    "    # save to json file\n",
    "    with open(os.path.join(context_folder, \"t5-train_context-summary-v1.json\"), 'w') as out_f:\n",
    "        js.dump(ent_context_dict, out_f, indent=4)\n",
    "        \n",
    "    print(f\"Triple {triple_cnt} processed...\")\n",
    "    print(\"========\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575c6070-f9d4-49fc-875a-fb310ed0283c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### test with training triples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f05ab7-78e3-46ea-9dbe-ef2d19c91e4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test with test triples\n",
    " - find neighbors\n",
    " - check images for obj and neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae8c7743-519f-4e91-a9ed-44683f2a6ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating head context for: Dominican Republic with 47 neighbors\n",
      "\t>> Checking neighbor: rel: /location/country/form_of_government, neighbor_subj: /m/06cx9, Republic\n",
      "\t>>>> Image: google_10.jpg | Yes: [0.940704, 0.84645], No: [0.058541, 0.152241]\n",
      "\t>>>> Generated context: a poster with a map of dominican republic\n",
      "\n",
      "\t>>>> Image: bing_17.jpg | Yes: [0.588609, 0.74861], No: [0.409313, 0.249711]\n",
      "\t>>>> Generated context: a poster that contains information about a republic\n",
      "\n",
      "\t>>>> Image: yahoo_9.jpg | Yes: [0.588609, 0.74861], No: [0.409313, 0.249711]\n",
      "\t>>>> Generated context: a poster that has republic dominican republic\n",
      "\n",
      "\t>> Related images found with neighbor Republic: 3\n",
      "\n",
      "Related images found for Dominican Republic: 3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate additional context for test entities\n",
    "data_folder = \"/blue/daisyw/ma.haodi/LLM-MMKGC/data/fb15k-237/\"\n",
    "test_triples_file = \"test.txt\"\n",
    "\n",
    "# triples\n",
    "test_triples = []\n",
    "with open(os.path.join(data_folder, train_triples_file)) as triple_f:\n",
    "    for line in triple_f.readlines():\n",
    "        obj_id, rel, subj_id = line.strip().split(\"\\t\")\n",
    "        obj_s, subj_s = ent2str_dict[obj_id], ent2str_dict[subj_id]\n",
    "        \n",
    "        # go with h, r, _ first\n",
    "        neighbors = train_ent_dict[obj_id]\n",
    "        print(f\"Generating head context for: {obj_s} with {len(neighbors)} neighbors\")\n",
    "        obj_img_add_context = []\n",
    "        for rel, nei_id in neighbors:\n",
    "            neighbor_context = []\n",
    "            nei_s = ent2str_dict[nei_id]\n",
    "            obj_path = os.path.join(image_folder, obj_id[1:].replace('/', '.'))\n",
    "            nei_path = os.path.join(image_folder, nei_id[1:].replace('/', '.'))\n",
    "            print(f\"\\t>> Checking neighbor: rel: {rel}, neighbor_subj: {nei_id}, {nei_s}\")\n",
    "             \n",
    "            # form prompts\n",
    "            # obj_prompt = f\"Is {obj_s} related to the image? Return Yes/No.\"\n",
    "            # nei_prompt = f\"Is {nei_s} related to the image? return Yes/No.\"\n",
    "            obj_prompt = (\n",
    "                            f\"Is the entity '{obj_s}' present in or related to this image? \"\n",
    "                            \"Respond with 'Yes' or 'No' only.\"\n",
    "                        )\n",
    "            nei_prompt = (\n",
    "                            f\"Is the entity '{nei_s}' present in or related to this image? \"\n",
    "                            \"Respond with 'Yes' or 'No' only.\"\n",
    "                        )\n",
    "            # context_prompt = f\"Describe the image in one sentence. Make sure you include {obj_s} and {nei_s}.\"\n",
    "            context_prompt = (\n",
    "                                f\"Analyze this image and check if it contains representations of both '{obj_s}' and '{subj_s}' from the knowledge graph. \"\n",
    "                                f\"Explain how these entities are depicted in the image or confirm if they are related to the content of the image in one sentence.\"\n",
    "                            )\n",
    "            prompt_list = [obj_prompt, nei_prompt, context_prompt]\n",
    "            \n",
    "            \n",
    "            # check relevant images and generate context\n",
    "            if os.path.exists(obj_path):\n",
    "                neighbor_context += related_ent_img_search(obj_path, model, processor, prompt_list)\n",
    "            if os.path.exists(nei_path):\n",
    "                neighbor_context += related_ent_img_search(nei_path, model, processor, prompt_list)\n",
    "\n",
    "            obj_img_add_context += neighbor_context\n",
    "            print(f\"\\t>> Related images found with neighbor {nei_s}: {len(neighbor_context)}\\n\")\n",
    "            break\n",
    "        print(f\"Related images found for {obj_s}: {len(obj_img_add_context)}\\n\\n\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gDINO",
   "language": "python",
   "name": "gdino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
